---
title: Introduction to machine learning - decision tree
author: ''
date: '2020-11-18'
slug: introduction-to-machine-learning-decision-tree
categories:
  - ML
  - Decision tree
tags:
  - Decision tree.
subtitle: 'Decision tree explained'
summary: ''
authors: []
lastmod: '2020-05-22T14:42:57+02:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      tidy = "styler", fig.width = 8, fig.height = 5)
library(tidyverse)
library(tidymodels)
library(tidyquant)
theme_set(theme_minimal())
```

# Introduction

This post describes a data driven method that are transparent and easy to interpret.
I am of course talking about devision trees. Here trees are based on separating records
into subgroups by creating splits on predictors. These splits are creates from
logical rules. Fx if age < 55 and edu > 12 then class =1. The results can be
seen as a more homogeneous in terms of the outcome variable and therefore being a
useful predictior. 

In the post I will go though two key concepts **recursive partitioning** and
**pruning**. For measuring the perormance I will go thoug some special metrices
that are used.

I am also gonna build a classification model based on a stocks.

# Decision tree

For many analysis if one should choose a framwork that work across a wide range
of situations one would pick the tree methodology. The best way of discribing the
tree is by an example. Let us say we have some bank data where we want to
classify the customers as either acceptors or nonacceptors based on; income, education
level, and average credit card expenditure. By using the tree method we can come
up with clear rules:

> if(income >106) and (education < 1.5) and (familiy < 2.5) then class = 0
(nonacceptor)

So what really goes on begind is that we grow a tree and with what we feed the
tree it starts to split up. But if you dont stop it the growth will not stop.

Let us turn two one of the two keyswords, **Recursive Partitioning**. Assume
we have a outcome variable Y and several predictores, $X_1,X_2,...,X_p$.
What recursive partitioning is about is we divides X into a p-dimensional space
this mean that we look at $X_i$ and its corresponding value, $s_i$. This X is 
splited up into two paths, one $X_i<s_i$ and the other $X_i>s_i$. Then one or 
both parts is divided by in the same manner by choosing another predictor. 
The idea is to split the entire X space up and get a possible pure record.

![Caption for the picture.](decision-tree.PNG)

This is of course not possible because every predictor as not a clear number
of records. Therfore wee need some measure for impority. So for every node we 
need to calculate how pure the node is. 

One measure is the **Gini index**. Denote *m* classes of the respose variable by
$k=1,2,...,m$. The Gini index for a split A is defined by

$$
I(A)=1-\sum^{m}_{k=1}p^2_k
$$
where $p_k$ is the proportion of records in split A that belong to class $k$.
This measure takes value between 0 (when all records belong to the same class)
and $\frac{m-1}{m}$ (when all *m* classes are equally represented). For the 
two classes case the Gini index would be at its peak when $p_k=0.5$ here the
split contains 50% of each of the two classes. 

Another useful measure is **entropy measure** whivh for split A is defined as

$$
entropy(A)= -\sum^{m}_{k=1}p_k \log_2(p_k).
$$
Again this measure ranges between 0 (most pure, all records belong to the same 
class) and $\log_2(m)$ (when all $m$ classes are represented equally).

When looking at the tree we have different nodes. One is called a *decision node*
and they are always followed by anoter node. If we have a new record then it will
drop down to a decision node and fall into one category until we come to the
*terminal node*. When the record reach a terminal node we have classified the
record. 

Working with decision tree gives two shortcomings. 

- Unstability: It is because a tree structure is unstable.
- A tree would lead to overfitting.

Again the danger with ovefitting is our model will perform bad at new data. One
way of handling this issue is **pruning**. This methods works where we remoed
the wealest branches so the tree is reduced in size. 

The pruning wont solve the first issue with the unstability but here we can
use cross validation. By using cross validation we split the traning data
into multiple samples and then pool the results. Here we can learn
how deep to grow the tree. For measuring how deep it should be we will introduce
**cost complexity (CC)** which is equal to its misclassification error plus a
penalty factor for the size of the tree. For a tree *T* that has *L(T)* terminal
nodes, the cost complexity can be written as

$$
CC(T)=err(T)+\alpha L(T)
$$
where *err(T)* is the fraction of traning records that are misclassified by tree T and $\alpha$
is a penalty factor for tree size. 
 
Three models has several advantages and are useful for both variable selection 
(the most important predictor will show uå above), no need for variable
transformation. The method is also robust to outliers because the splits depends
on the ordering and not the magnitudes.

One advantages but also a shortcomings is the nonlinearity and non parametric
with the model. The strengt is it allow a wide range of relationships between
the predictors and outcome variable. This is how ever also a weakness because we
might miss relationsship between predictors and the outcome variable. 

Another problem is thaat it require large dataset to cnstruct a good classifier.
It can also be heavy to run on a computer becasue we need to computing all
possible plits. 

While the model has it advantages and short commings it is consider to be
a good method with gives help in both classification and regressions tasks.

# Model 



```{r}
sf_trees <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-28/sf_trees.csv")

```

```{r}
sf_trees %>% glimpse()
```

We have turbine capacity and want to build a model around that.

```{r}
sf_trees %>%  count(legal_status, sort = TRUE)
```
```{r}
df <- sf_trees %>% 
  mutate(legal_status = case_when(
    legal_status == "DPW Maintained" ~ legal_status,
    TRUE ~ "Other"),
    plot_size = parse_number(plot_size)) %>% 
  select(-address) %>% 
  na.omit() %>% 
  mutate(across(where(is.character), as.factor))
```

```{r}
df %>% skimr::skim()
```

```{r}
df %>% 
  ggplot(aes(longitude, latitude, color = legal_status)) +
  geom_point(size = 0.5,alpha = 0.4) +
  labs(color = NULL)
```

We can use the lat and long to our model. 

```{r}
df %>% 
  count(legal_status, caretaker) %>% 
  add_count(caretaker, wt = n, name = "caretaker_count") %>% 
  filter(caretaker_count > 50) %>% 
  group_by(legal_status) %>% 
  mutate(percent_legal = n/ sum(n)) %>% 
  ggplot(aes(percent_legal, caretaker, fill = legal_status)) +
  geom_col(position = "dodge")
```



It is character and it should be a numeric variable. 

## Build models

```{r}
set.seed(123)

split = initial_split(df, strata = legal_status)

df_train <-training(split) 
df_test <-  testing(split)
```

### prepo

```{r}
df_rec <- 
  recipe(legal_status~ ., data = df_train) %>% 
  update_role(tree_id, new_role = "ID") %>% 
  step_other(species, caretaker, threshold = 0.01) %>%  # collaps categorical levels. there is three. Can set s threshole 
  step_other(site_info, threshold = 0.002) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_date(date, features = c("year")) %>% # make new features
  step_rm(date) %>% 
  step_downsample(legal_status)

# date column dosent like dates so we exlude it. 
# recipe is sayubg what is goin to happen
# prep goes to tain and do it.

df_prep <- prep(df_rec) # her we have made prpcessing
# If you want to look at it then we use juice
jiced <- juice(df_prep)
```
We could choose to tune the steps but for this exercies we onlu tune 
the parameter. 

We have now specified the recipe but not yet train it on a model

Now we gonna build our model. 

spec stand for specifiction

```{r}
tune_spec <- rand_forest(
  mtry = tune(), 
  trees = 1000, # have enofgut
  min_n = tune() # how long do we keep splitting.
) %>% 
  set_mode("classification") %>% 
  set_engine("ranger")
```

Have defined the model

now we gonna set up the workflow

```{r}
tune_wf <- 
  workflow() %>% 
  add_recipe(df_rec) %>% 
  add_model(tune_spec)
```



## train hyperparamet

cross val

```{r}
set.seed(124)
folds <- vfold_cv(df_train)

doParallel::registerDoParallel()
set.seed(345)
tune_res <- 
  tune_grid(
  tune_wf,
  resamples = folds,
  grid = 20
)
```

Have 10 cross validation folds. 

When looking at the output we can see that we are gonna train some path of the
data and leave a bit out to make a prediction. 

 We don´t keep tje models but only the 
 
```{r}
tune_res  %>%  select_best("accuracy")
```

let us try to visual

```{r}
tune_res %>% 
  collect_metrics() %>% 
  filter(.metric == "roc_auc") %>% 
  select(mean, min_n, mtry) %>% 
  pivot_longer(min_n:mtry,
               values_to = "value",
               names_to = "parameter") %>% 
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE)+
  facet_wrap(~ parameter, scales = "free_x")
  
```

We have tune on the parameter. Areal under the curve is low at first

Here we get an idea of which values we should pick. 

we need to tune again using the above information

```{r}
# use regular grid

rf_grid <- grid_regular(
  mtry(range = c(10,20)),
  min_n(range = c(2, 8)),
  levels = 5)

set.seed(345)
regular_res <- 
  tune_grid(
  tune_wf,
  resamples = folds,
  grid = rf_grid
)
```

Took the uådated grid and train a buch of models that a tunes. 

```{r}
regular_res %>% 
  collect_metrics() %>% 
  filter(.metric == "roc_auc") %>% 
  mutate(min_n = factor(min_n)) %>% 
  ggplot(aes(mtry, mean, color = min_n))+
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() 
```

```{r}
best_auc <- select_best(regular_res, "roc_auc")

# use function
final_rf <- finalize_model(
  tune_spec,
  best_auc
)
```

Now we have the best model to test.

```{r}
library(vip)

# variable importance

final_rf %>% 
  set_engine("ranger",
             importance = "permutation") %>% 
    fit(legal_status ~ .,
        data = juice(df_prep) %>% select(-tree_id)) %>% 
  vip(geom = "point")
```

take final model anset engien and fit to the traning data one time. Her we
make a point that indicate if we have the right predictors to our model. 


now turn our eyes to the test data to see how well our model does
```{r}
final_wf <- 
  workflow() %>% 
  add_recipe(df_rec) %>% 
  add_model(final_rf)

# last fit is when you have done the tuning and are ready to look at
# the test set. 
final_res <- 
  final_wf %>% 
  last_fit(split)

final_res %>% 
  collect_metrics()
```

didt not overfit under tuning. 

```{r}
final_res %>% 
  collect_predictions() %>% 
  mutate(correct = case_when(
    legal_status == .pred_class ~ "Correct",
    TRUE ~ "Incorrect")) %>% 
  bind_cols(df_test) %>% 
  ggplot(aes(longitude, latitude, color = correct)) +
  geom_point(size = 0.5,alpha = 0.5) +
  labs(color = NULL) +
  scale_color_manual(values = c("gray80", "darkred"))
```

 

## Explore results



