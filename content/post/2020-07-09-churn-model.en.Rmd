---
title: Churn model
author: Lucas Bagge
date: '2020-07-09'
slug: churn-model
categories:
  - tidymodels
  - churn
  - plotly
tags:
  - tidymodels
  - churn
  - plotly
summary: 'I denne post vil jeg lave en churn model og se på tidymodels framwork til at vælge forskellige modeller.'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      tidy = "styler", fig.width = 3, fig.height = 4)


# Modelling AlgorithmsS
library(glmnet) # Glmnet regression
library(ranger) # Random Forests

# Formating, Visualisations and tables
library(scales) # Number formats
library(knitr) # Table
library(gridExtra) # multiplot
library(e1071) # Summary distribution
library(skimr) # Summarise dataframe
library(corrplot) # Correlation plot
library(probably) # Probability thresholds
library(tidymodels)
library(caret)
# Data handling Packages
library(tidyverse) # Data handling/ Graphics
library(data.table) # Data handling
library(plotly)
```


# Introduktion

Det primære formål ved denne post er at undersøge `tidymodels` prediktive model framwork.
Max Kuhn skaberen af `caret` har erstattet den med tidymodels og er nu den store models
pakke, som bruges rund om i verden. For at undersøge workflow og underkomponenterne
vil jeg lave en churn model over et data sæt jeg har stødt på hos **Kaggle**.

Det specielle workflow fra tidymodels bygger på nogle nøgle pakker:

* `rsample` - Forskellige typer af resamples
* `recipes` - Transformation af modellens pre processiong 
* `parnip` - Et interface for model skabelse
* `tune` - Tune hyperparameter
* `dials` - Specifik hyperparameter tuning funktion
* `yardstick` - Måling af modellens præsentation

Ud over hele tidymodels framworked vil jeg og forsøge mig med at lave interaktive plots, ved hjælpm af `plotly`. 

# Data 

Det første er at loade data ind. Her har jeg fundet ud af at i stedet for at bruge
`base` `read.csv()` kan man for bedre indlæsning, der er mere konsistent, med
`fread` funktionen.

```{r}
df_churn <- data.table::fread("C:/Users/LUCBA/Projects/Churn_Modelling.csv")
```

# Exploratory Data Analysis (EDA)

For at danne sig et overblik over data bruges funktionen `skim()`, som viser vi har
 `r comma(nrow(df_churn))` observationer og `r ncol(df_churn)` variabler.
Der er 3 character variabler and 11 numeriske.

Character variablerne relaterer sig til køn, geografi og surname. De numeriske 
har to reference id. Derudover relateres de sig til 

- age,
- tenure,
- credit,
- income,
- exited (vores variabel der indikere om man har **churned**)

```{r}
# Summarise datafrmae
skim(df_churn)
```

Skim outputter viser at de fleres variable navne har et mix af øvre og nedre  upper and lower case characters
Her bruges set_names funktionen til at konverter navne til lower case. For de specifikke 
identifier så fjernes de da, man ikke har brug for dem.

```{r warning = FALSE, message = FALSE}

# Convert all names to lower case
df_churn <-
  df_churn %>%
    set_names(., tolower(names(.))) %>%
    select(-c(rownumber, customerid))
```

## Exploring Target Variable

Vores outcome variable af **exited**, grafen viser at omkring 80% af kunderne bliver
mens 20% churns. Det er altså et ubalancerede data, så det skal behandles en anden gang.
Exited er en numerisk variable, men den bliver konverterede til en faktor.


```{r echo= FALSE, warning = FALSE, message = FALSE, fig.align = "center", fig.height=2, fig.width=2}
theme_set(theme_minimal())
# Visualise the distribution of the data
plotly::ggplotly(
df_churn %>%
  select(exited) %>%
  mutate(exited = factor(exited, levels = c(0,1), labels = c("Remain", "Churn"))) %>%
  group_by(exited) %>%
  count() %>%
  ungroup() %>%
  mutate(p = n / sum(n)) %>%
  ggplot(aes(x = exited, y = p)) +
    geom_col(fill = "blue") +
    scale_y_continuous(labels = scales::percent) +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(x = NULL, y = NULL) +
    ggtitle("Proportion of Loan Repayments & Default")
)
```

```{r}
# Change the variable type from numeric to factor
df_churn <- 
  df_churn %>% 
    mutate(exited = factor(exited, levels = c(1,0), labels = c("Churn", "Remain")))

```


## De kategoriske variabler

Den nedenstående graf indikerer at mænd har en lavere risiko for at churne end kviner.
Kigger vi på geografien er der ikke den store forksel mellem Frankrig og Spanien.
Dog hvis vi sammenligner med Tyskland, så er der lavere sandsynlighed for de churner.

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.align="center", fig.height=2, fig.width=2}

library(patchwork)

# Distribution by Geography
p1 <- plotly::ggplotly( 
  df_churn %>%
    group_by(geography, exited) %>%
    count() %>%
    ggplot(aes(x = geography, y = n, fill = exited)) +
    geom_col(position = "fill") +
    scale_y_continuous(labels = scales::percent) +
    labs(y = NULL, x = NULL) +
    theme(plot.title = element_text(hjust = 0.5),
          legend.position = "bottom") +
    ggtitle("Geography")
)

# Distribution by Gender
p2 <- plotly::ggplotly(
  df_churn %>%
    group_by(gender, exited) %>%
    count() %>%
    ggplot(aes(x = gender, y = n, fill = exited)) +
    geom_col(position = "fill") +
    scale_y_continuous(labels = scales::percent) +
    labs(y = NULL, x = NULL) +
    theme(plot.title = element_text(hjust = 0.5),
          legend.position = "bottom") +
    ggtitle("Gender")
)
subplot(p1, p2, nrows = 1)

```

## Udforskning af de numeriske variabler

Plottet kigger på forholdet mellem hver numerisk variable og churn. To af disse
"hascrcard" og "isactivemember" er binære variable og skal ikke visualiseres.
De andre variabler er:

* `Age`: Det ser ud til at der er et forhold mellem kunder der churner er ældre end gennemsnittet der ikke gør
* `Balance`: Gennemsnit af ens, men dem som churn ser ud til at have en højere balance.
* `Credit score`: Der er ikke den store forskel i forhold til credit scoren.
* `Estimated salary`: Igen ikke den store forskel.
* `Number of products`: Ikke den store forskel igen.
* `Tenure`: Der er en større fordeling for kunder de churn. Det ser ud til at nye og gamle kunder har større sandsynlighed for at churn.

```{r warning = FALSE, message = FALSE, fig.align="center", fig.height=2, fig.width=2}

# Relationship with Churn and Numerical variables
plotly::ggplotly(
  df_churn %>%
  {bind_cols(select_if(., is.numeric),
             select_at(., "exited"))
  } %>%
  gather(-exited, key = "var", value = "value") %>%
  ggplot(aes(x = exited, y = value, fill = exited)) +
    geom_boxplot() +
    theme(legend.position = "none") + 
    facet_wrap(~ var, scales = "free")  +
    ggtitle("Numerical Variable Relationship with Churn")
)
```

For `credit card` og `Active member` variablerne skal de ikke vivsualiseres
med et boxplot. Først skal de konverteres til faktor og så kan vi plotte dem og se om der er et forhold. For credit card ser det ikke ud til at påvirke om man churn eller ej. Alternativt ser det ud til at brugerens aktivitets niveau har en betydning.

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.align="center", fig.height=2, fig.width=2}

# Distribution by Credit Card
p1 <- ggplotly(df_churn %>%
                 mutate(hascrcard = factor(hascrcard, labels = c("No Card", "Has Card"))) %>%
                 group_by(hascrcard, exited) %>%
                 count() %>%
                 ggplot(aes(x = hascrcard, y = n, fill = exited)) +
                 geom_col(position = "fill") +
                 scale_y_continuous(labels = scales::percent) +
                 labs(y = NULL, x = NULL) +
                 theme(plot.title = element_text(hjust = 0.5),
                       legend.position = "bottom") +
                 ggtitle("Credit Card")
)

# Distribution by isactivemember
p2 <- ggplotly(df_churn %>%
                 mutate(isactivemember = factor(isactivemember, labels = c("Inactive", "Active"))) %>%
                 group_by(isactivemember, exited) %>%
                 count() %>%
                 ggplot(aes(x = isactivemember, y = n, fill = exited)) +
                 geom_col(position = "fill") +
                 scale_y_continuous(labels = scales::percent) +
                 labs(y = NULL, x = NULL) +
                 theme(plot.title = element_text(hjust = 0.5),
                       legend.position = "bottom") +
                 ggtitle("Active Member")
)
subplot(p1, p2, nrows = 1)
```


Plotter viser at der ikke er en korrelation mellem de numeriske værider, så vi skal ikke tænker på at håndtere **multicollinaritet**

```{r warning = FALSE, message = FALSE, fig.align="center"}
# Create a variable of total family size
corrplot(cor(df_churn %>% select_if(is.numeric)))
```


# Feature Engineering

En vigtig del ved hver machine learning projekt er feature engineering, som er med til at forbedre præcisionen af modellen. Da dette projekt har trukket tænder ud og min afsnit og variabler ikke har stem helt overens med mine forventninger, så springer vi dette afsnit over, men det er helt sikkert noget der burde addreseres i et review af analysen. 

# Model Building

Nu hvor er data klar til at blive modelleres og man kan for alvor se på hvordan tidymodels fungerer. Der vil bliver bygget forskellige modeller til at forudsige churn, og nogle af disse skal tunes og evaluseres ved brug af **ROC metricen**. De følgende modeller er:

* GLM
* GLMNET
* Random Forest

Her vil man i denne sektion udforske følgende:

* Split data i træning og test.
* Bestemme hvile præ processer step der kræves. 
* Bygge, sammenligne og vælge den bedste model.
* Beregne sandsynligheds grænser til at bestemme den optimale cut off.
* Apply best model to test data and calculate evaluation statistics
* Compare standard cut off to calculated threshold

## Split Data

Det er vigtig at sikrer sig at ens model ivaluerer over det et korrekt antal test og trænings data, her kan `inital_split` funktionen bruges fra `rsample`. I funktionen kan man specificerer det stratificerede sample med ens udkoms variable for at sørge for at fordelingen er ens i de to samples. Tabellen forneden viser at vi har fået samme fordeling i begge sæt.


```{r warning = FALSE, message = FALSE}

# Create an index to split the data
set.seed(1989)
l_split_index <- 
  initial_split(data = df_churn,
                prop = 0.75,
                strata = exited
                )

# Create training and testing set using the index
df_train <- training(l_split_index)
df_test <- testing(l_split_index)
  
# Check same distribution of output
bind_rows(
          as.data.frame(round(prop.table(table(df_train$exited)),4)) %>% mutate(Data = "Train"),
          as.data.frame(round(prop.table(table(df_test$exited)),4)) %>% mutate(Data = "Test")
          ) %>%
  spread(Var1, Freq) %>%
  kable(style = "pandoc",
        align = c('c','c','c'))

```

## Pre processing data

Efter dette initiale step kan vi bruge `recipe` til at lave en præprocess hvor vi transformerer data, som kan være med til at forbedre modellen evne til forudsigelser. Som navnen indikere er recipe bygget op imkring det at bage. Nøgle funktionerne er:


* `recipes`: De præ processer vi ænsker der skal bruges i vores data.
* `prep`: Estimaterer  parameterne associerede med den valge recipe.
* `bake`: Bager de to ovensteådende skridt.


De følgende recipe steps vil blive brugt:

* Nominelle variabler konvertes til dummy variabler.
* Center og skaler alle numeriske værider.
* Fjerne høj korrelerede variabler.
* Fjerne variable med ingen varians.
* Fjerne variabler med min varians.

Når recipe er laver skal bake funktionen bruges til at transformerer træning og test sæettet og erstatte det gamle data sæt. 


```{r warning = FALSE, message = FALSE}

# Preprocessing - With near zero inflation
recipe <-
  df_train %>%
    recipe(exited ~ .) %>%
    step_rm(surname) %>%
    step_dummy(all_nominal(), -all_outcomes()) %>%
    step_normalize(all_numeric(), -all_outcomes()) %>%
    step_corr(all_numeric(), -all_outcomes(), threshold = 0.9) %>%
    step_zv(all_predictors()) %>%
    step_nzv(all_predictors()) %>%
    prep()

# Apply processing to test and training data
df_baked_train <- recipe %>% bake(df_train) # Preprocessed training
df_baked_test <- recipe %>% bake(df_test) # Preprocessed testing

rm(df_train, df_test) # remove old data

```

## Cross Validation



Cross validation går vi kan undersøge flere skæringer af vores datA og opbygge en tro og sikkerhed i at modellens præsentation er god og dermed reducer **bias** og **overfitting**. Der er forskellige antalg **folds** man kan vælge, men valget ender med 5, da det oftes er 5 eller 10 man står og vil vælge mellem.

Man kan benytte sig af `vfold_cv` funtionen til at lave 5 lige store splits i vores baked træningssæt.

```{r warning = FALSE, message = FALSE}
# Cross validation
set.seed(1989)
l_cv <- vfold_cv(df_baked_train, v = 5, strata = "exited") # Cross validation
```

## De forskellige modeller + modeller med hyperparameter

For at teste flexibiliteten af tidumodels vil de følgende tre modeller anvendes:
To test the flexibility of tidymodel, the following three models will be used: 

* `glm`
* `glmnet`
* `random forest`

For hver model vil 5 modeller blive bygget på datasplittet skabt af rsamples for cross validation. glmnet og random forest har hyperparameter, så funktionaliteterne fra `tune` og `dials` vil blive brugt for at undersøge de bedste hyperparameter med respekt til roc.

### GLM

Den logistiske regression model er en basal, men effektiv klassifikations model, når den er bygget korrekt. Da denne post er tænkt som en visning af tidymodels, vil de variablerne som er signifikantet blive inkluderet i denne baseline model. Tabellen viser de variabler der medtages i modellen.

```{r warning = FALSE, message = FALSE}

# Get features
logistic_reg(mode = "classification") %>%
  set_engine("glm") %>%
  fit(exited ~ ., data = df_baked_train) %>%
  tidy() %>%
  filter(p.value < 0.05) %>%
  kable(align = c('c', 'c', 'c', 'c', 'c'))

```

Funktionen forneden bruger `purrr::map2_df()` til at itererer over flere faktor og returner en dataframe. Det gøres over vores **l_cv**.

I hver iteration:

* Vil analyse og assessment funktion blive brugt til at splitte data effektiv i et træning og test sæt. Der er en sammenlignihed med traning og testning, men man bruger førnævnte betegnelse, når vi arbejder i cross validation.
* En GLM model bliver bygge på **analysis data**, med de kolonner i tabellen foroven.
* GLM modellen bruges til at forudsige outcome variable i **assessment data**, både klassen og sandsynligheden bevares i en tabel.
* Output opbevares i en liste til brug sensere i analysis.

```{r warning = FALSE, message = FALSE}
# glm - 5 fold CV
mod_glm <-
  list(parameters = NULL,
       df = map2_df(.x = l_cv$splits,
                    .y = l_cv$id,
                    function (split = .x, fold = .y) 
                       {
                         # Split the data into analysis and assessment tables
                         df_analysis <- analysis(split)
                         df_assessment <- assessment(split)
                         
                         # Build the model
                         mod <-
                          logistic_reg(mode = "classification") %>%
                          set_engine("glm") %>%
                          fit(exited ~ creditscore + age + tenure + 
                                       balance + isactivemember + 
                                       geography_Germany + gender_Male, 
                              data = df_analysis)
                         
                         # Summarise Predictions
                         table <- 
                           tibble(fold = fold,
                                  truth = df_assessment$exited,
                                  .pred_Churn = 
                                      predict(mod, 
                                              new_data = df_assessment, 
                                              type = "prob")[[".pred_Churn"]],
                                  .pred_Remain = 
                                      predict(mod, 
                                              new_data = df_assessment, 
                                              type = "prob")[[".pred_Remain"]],
                                  .pred_Class = 
                                      predict(mod, 
                                              new_data = df_assessment) %>% 
                                    unlist() %>% 
                                    as.character()
                                  ) %>%
                           mutate(.pred_Class = factor(.pred_Class))
                         }) 
       )
```

### GLMNET

Den anden model er en udvidelse af GLM modellen og kendes under, **aselastic net logistic regression**. Denne algoritme tilføjer en regulaisering til at kombiner **ridge** og **lasso** regression, som kontroller for størrelsen af modellen. Denne metode har to hyperparameter **Penalty** og *Mixture**, som indebærer vi skal tilføje et step i processen.

De ekstra steps er:

* Indenfor opbygningen af modellen vil penalty and mixture blive sidestillet med en tune funktion. Det betyder at man vil søge på tværs af disse parameter.

* Det andet skridt er at bruge `tune grid`, paramerter og `grid_max_entropy` fra tune og dials pakken.
  * `grid_max_entropy` skaber et grid af størrelsen 50 i dette tilføje af random parameter som dækker over parameterne i search spacet.
  * `tune_grud` tager en model formular på tværs af cross validering og en metric for at optimerer (roc i dette tilføje).
  * `select_best` fra tune pakken bruges til at tage de hyperparameter der giver det bedste resultat.

Når modellen er bygget genbruges den til cross validation og prediction og parameterne opbevares i en liste for senere brug.

```{r warning = FALSE, message = FALSE}

# Set the model engine
mod <-
  logistic_reg(mode = "classification",
    penalty = tune(), 
    mixture = tune()) %>%
    set_engine("glmnet") 

# Build initial model with varying parameters and cross validation
set.seed(1989)
mod_results_tbl <- 
  tune_grid(
            formula   = exited ~ .,
            model     = mod,
            resamples = l_cv,
            grid      = grid_max_entropy(parameters(penalty(), 
                                                    mixture()), 
                                         size = 50),
            metrics   = metric_set(roc_auc),
            control   = control_grid(verbose = FALSE)
      )

# Store the parameters
df_parameter <- mod_results_tbl %>% select_best("roc_auc")

# glmnet - 5 fold CV
mod_glmnet <-
  list(parameters = df_parameter, 
       df = map2_df(.x = l_cv$splits, # Add predictions and find c
                    .y = l_cv$id,
                    function (split = .x, fold = .y)
                     {
                       # Split the data into analysis and assessment tables
                       df_analysis <- analysis(split)
                       df_assessment <- assessment(split)
  
                       # Build the model
                       mod_2 <-
                        logistic_reg(mode = "classification",
                                     penalty = 
                                         as.numeric(df_parameter["penalty"]),
                                     mixture = 
                                         as.numeric(df_parameter["mixture"])
                                     ) %>%
                         set_engine("glmnet") %>%
                         fit(exited ~ ., data = df_analysis)
  
                       # Summarise Predictions
                       table <-
                         tibble(fold = fold,
                                truth = df_assessment$exited,
                                .pred_Churn = 
                                    predict(mod_2,
                                            new_data = df_assessment,
                                            type = "prob")[[".pred_Churn"]],
                                .pred_Remain = 
                                    predict(mod_2,
                                            new_data = df_assessment,
                                            type = "prob")[[".pred_Remain"]],
                                .pred_Class = 
                                    predict(mod_2, new_data = df_assessment) %>%
                                  unlist() %>%
                                  as.character()
                                ) %>%
                         mutate(.pred_Class = factor(.pred_Class))
                      })
    )
          
rm(mod, mod_results_tbl, df_parameter) # Clear memory
```

### Random Forest

Den sidste model er en random forest. Den har også hyperparameter og skal derfor tunes. 

Disse skridt er:

* Laver et model objekt, sætter hyperparameterne til at tune og sætter engine til ranger. Det betyder vi bruger random forest fra pakken `ranger`.

* grid_random bruges til at lave 50 random kombinationer af parameterne; `mtry`, `trees` og `min_n`. mtry kræver en øvre grænse, som i dette tilfælde er antallet af predictor.
* De bedste parameter bevares og bruges i cross validation.
* Output bevares i en liste til brug senere.

```{r warning = FALSE, message = FALSE}

# Set the model engine
mod <-
 rand_forest(mode = "classification",
             mtry = tune(),
             trees = tune(),
             min_n = tune()) %>%
set_engine("ranger") 

# Build initial model with varying parameters and cross validation
set.seed(1989)
mod_results_tbl <- 
  tune_grid(
    formula   = exited ~ .,
    model     = mod,
    resamples = l_cv,
    grid      = grid_random(parameters(mtry(c(1, 22)), 
                                       trees(), 
                                       min_n()), 
                            size = 50),
    metrics   = metric_set(roc_auc),
    control   = control_grid()
      )

# Store the parameters
df_parameter <- mod_results_tbl %>% select_best("roc_auc")

# random forest - 5 fold CV
mod_rf <-
  list(parameters = df_parameter,
       df = map2_df(.x = l_cv$splits,
                    .y = l_cv$id,
                    function (split = .x, fold = .y)
                     {
                       # Split the data into analysis and assessment tables
                       df_analysis <- analysis(split)
                       df_assessment <- assessment(split)
  
                       # Build the model
                       mod_2 <-
                        rand_forest(mode = "classification",
                                     mtry = as.numeric(df_parameter["mtry"]),
                                     trees = as.numeric(df_parameter["trees"]),
                                     min_n = as.numeric(df_parameter["min_n"])
                                     ) %>%
                         set_engine("ranger") %>%
                         fit(exited ~ ., data = df_analysis)
  
                       # Summarise Predictions
                       table <-
                         tibble(fold = fold,
                                truth = df_assessment$exited,
                                .pred_Churn = 
                                    predict(mod_2,
                                            new_data = df_assessment,
                                            type = "prob")[[".pred_Churn"]],
                                .pred_Remain = 
                                    predict(mod_2,
                                            new_data = df_assessment,
                                            type = "prob")[[".pred_Remain"]],
                                .pred_Class = 
                                    predict(mod_2, new_data = df_assessment) %>%
                                  unlist() %>%
                                  as.character()
                                ) %>%
                         mutate(.pred_Class = factor(.pred_Class))
                        })
          )

rm(mod, mod_results_tbl, df_parameter) # Clear memory
```

## Model Evaluation metrics

Denne sektion summaer præsentationen af de tre modeller fra vores cross validation og hyperparameter tuning i sektion foroven. Evaluations metricerne som kan ses forneden er:


* **accuracy**: Andelen af observationer modellen korrekt forudsiger.
* **sensitivity (true positive rate)**: Andelen af **actual positives** der er korrekte.
* **specificity (true negative rate)**: Andelen af  **actual negatives** der er korrekte.
* **auroc**: Måler hele arealet under ROC kurven, som måler modellen evne til at klassificere ved hver grænseværdi.

Plottet viser:

* Konsisten over de forskellig metricer at RD er bedst.
* GLM viser gode resultater sammenlignet med GLMNET.
* Alle modeller har en højere **specificity** end **sensitivity**, som måske skyldes imbalance i vores data.

```{r echo= FALSE, warning = FALSE, message = FALSE, fig.align = "center", fig.height=2, fig.width=2}
# Extract the cross validated predictions
mod_summary_all <-
  bind_rows(mod_glm[["df"]] %>% mutate(model = "glm"),
            mod_glmnet[["df"]] %>% mutate(model = "glmnet"),
            mod_rf[["df"]] %>% mutate(model = "randomforest"))

# Summarise evaluation stats  
lapply(1:5, function(x)
  {
  bind_rows(
            # -- Accuracy
    
            # GLM
            mod_summary_all %>% 
              filter(fold == paste("Fold", x, sep = "") & model == "glm") %>%
              conf_mat(truth = truth, estimate = .pred_Class) %>%
              summary() %>%
              filter(.metric %in% c("accuracy", "sens", "spec")) %>%
              mutate(fold = x,
                     model = "glm"),
            
            # GLMNET
            mod_summary_all %>% 
              filter(fold == paste("Fold", x, sep = "") & model == "glmnet") %>%
              conf_mat(truth = truth, estimate = .pred_Class) %>%
              summary() %>%
              filter(.metric %in% c("accuracy", "sens", "spec")) %>%
              mutate(fold = x,
                     model = "glmnet"),  
            
            # Random Forest
            mod_summary_all %>% 
              filter(fold == paste("Fold", x, sep = "") & model == "randomforest") %>%
              conf_mat(truth = truth, estimate = .pred_Class) %>%
              summary() %>%
              filter(.metric %in% c("accuracy", "sens", "spec")) %>%
              mutate(fold = x,
                     model = "randomforest"),  

            # -- ROC  
            
            # GLM
            mod_summary_all %>% 
              filter(fold == paste("Fold", x, sep = "") & model == "glm") %>%
              roc_auc(truth, .pred_Churn) %>%
              mutate(fold = x,
                     model = "glm"),
            
            # GLMNET
            mod_summary_all %>% 
              filter(fold == paste("Fold", x, sep = "") & model == "glmnet") %>%
              roc_auc(truth, .pred_Churn) %>%
              mutate(fold = x,
                     model = "glmnet"),            
             
            # GLMNET
            mod_summary_all %>% 
              filter(fold == paste("Fold", x, sep = "") & model == "randomforest") %>%
              roc_auc(truth, .pred_Churn) %>%
              mutate(fold = x,
                     model = "randomforest")
            )}) %>%
  rbindlist() %>%
  arrange(fold, model, .metric) %>% 
  ggplot(aes(x = fold, y = .estimate, col = model)) +
    geom_point() +
    facet_grid(. ~ .metric) +
    scale_y_continuous(labels = percent, breaks = seq(0, 1, 0.1)) +
    theme(legend.position = "top",
          plot.title = element_text(hjust = 0.5)) +
    ggtitle("Prediction Evaluation Statistics ")

```


## Sandsynligheds grænse

En metode for at addreserer ubalancer i vores kasse (churn og remain er andelsmæssig er forskellig) er at justerer grænsen der angiver sandsynligheden for churn på remain. Grænsen sættes typisk til 50 %, men vi kan sætte en grænse der giver en bedre balance mellem sensitivyty og specificuty. I dette tilfælde skal modellen estimerer churn eller remain på lige fod, så et ny grænse er ønsket.

Koden forneden forsøger netop det ved at bruge `threshold_perf` fra `probably`. Her kan revaluering metricerne blive beregnet for hver 1 % mellem 0 og 1, hvor grænsen ændres. Analysen viser at en grænseværdi på 20% vil give et mere præcis forhold mellem sensitivity og specificity.


```{r warning = FALSE, message = FALSE}

# probability thresholds by fold
df_thresholds <-
  mod_summary_all %>%
    filter(model == "randomforest") %>%
    select(fold, truth, .pred_Churn) %>%
    group_by(fold) %>%
    probably::threshold_perf(truth, .pred_Churn, thresholds = seq(0, 1, 0.01)) %>% 
    filter(.metric %in% c("spec", "sens", "distance")) %>%
    spread(.metric, .estimate) %>%
    mutate(distance_abs = abs(sens - spec)) %>%
    group_by(fold) %>%
    filter(distance_abs == min(distance_abs)) %>%
    ungroup() 

# Show table
df_thresholds %>% kable(align = c('c', 'c', 'c', 'c', 'c', 'c', 'c', 'c'))
```

Tag gennemsnittet af ovenstående cross validation, så får vi en gennemsnitlig grænse på 20.2%. Det betyder altså at hvis vi bruger grænsen på 50 % vil vi reducer præcisionen af hvor gode vi er til at forudsige de kunder som bliver, men vi vil forbedre hvor godt vi kan forudsige kunder der churn.

```{r warning = FALSE, message = FALSE}
# Summary threshold
df_thresholds <-
  df_thresholds %>%
      summarise(.threshold = mean(.threshold),
                distance = mean(distance),
                sens = mean(sens),
                spec = mean(spec),
                distance_abs = mean(distance_abs))

# Show table
df_thresholds %>% kable(align = c('c', 'c', 'c', 'c', 'c', 'c', 'c', 'c'))

```

# Evaluering af forudsigelse

Den sidste del af analysen er at bruge modellerne til at se hvor gode de er til at forudsige test data. Her vil vi bruger alle tre modeller til at se om kunder churn eller reamin i vores testing sæt.

```{r warning = FALSE, message = FALSE}
# Final glm
mod_final_glm <-
  logistic_reg(mode = "classification") %>%
    set_engine("glm") %>%
    fit(exited ~ ., df_baked_test)

# Final glmnet
mod_final_glmnet <-
  logistic_reg(mode = "classification",
               penalty = mod_glmnet[["parameters"]][["penalty"]], 
               mixture = mod_glmnet[["parameters"]][["mixture"]]
               ) %>%
    set_engine("glmnet") %>%
    fit(exited ~ ., df_baked_test)


# Final random forest
mod_final_rf <-
 rand_forest(mode = "classification",
             mtry = mod_rf[["parameters"]][["mtry"]],
             trees = mod_rf[["parameters"]][["trees"]],
             min_n = mod_rf[["parameters"]][["min_n"]]
             ) %>%
  set_engine("ranger") %>%
  fit(exited ~ ., df_baked_test)
```

Tabellen forneden viser  præsentationen af modellerne på test data:

* The random forest model klarer det bedst både på auroc og accuracy
* Glm og glmnet deler vandene

Random forest klarer det bedst og vil derfor være den model vi vælger.

```{r echo = FALSE, warning = FALSE, message = FALSE}

# Summary of prediction performance
bind_rows(
          predict(mod_final_glm, new_data = df_baked_test, type = "prob") %>%
            bind_cols(predict(mod_final_glm, new_data = df_baked_test)) %>%
            bind_cols(df_baked_test) %>%
            metrics(exited, .pred_Remain, estimate = .pred_class) %>%
            mutate(model = "glm"),
          
          predict(mod_final_glmnet, new_data = df_baked_test, type = "prob") %>%
            bind_cols(predict(mod_final_glmnet, new_data = df_baked_test)) %>%
            bind_cols(df_baked_test) %>%
            metrics(exited, .pred_Remain, estimate = .pred_class) %>%
            mutate(model = "glmnet"),
          
          predict(mod_final_rf, new_data = df_baked_test, type = "prob") %>%
            bind_cols(predict(mod_final_rf, new_data = df_baked_test)) %>%
            bind_cols(df_baked_test) %>%
            metrics(exited, .pred_Remain, estimate = .pred_class) %>%
            mutate(model = "randomforest")
          ) %>%
  filter(.metric %in% c("accuracy", "roc_auc")) %>%
  spread(.metric, .estimate) %>%
  select(-.estimator) %>%
  kable(align = c('c', 'c', 'c'))
```

Til sidst kan man sammenligne forudsigelser af random forest med standard grænsen på 50 % med den beregnet grænse på 20.2%. Vi ser at sensitivity er steget dramatisk og specificity er reduceret. Min forventning ville have været at disse to metricer havde været ens, men i og med det ikke er tilfældet indikerer at der er bias mellem testing og træning data.

```{r echo = FALSE, warning = FALSE, message = FALSE}

# Comparison of evaluation at 50% and calculate thresholds
bind_rows(
          # 50% cut off
          predict(mod_final_rf, new_data = df_baked_test) %>%
            bind_cols(df_baked_test) %>%
            conf_mat(truth = exited, estimate = .pred_class) %>%
            summary() %>%
            filter(.metric %in% c("accuracy", "sens", "spec")) %>%
            mutate(.cutoff = "50_perc"),
          
          # Balanced cut off
          predict(mod_final_rf, new_data = df_baked_test, type = "prob") %>%
            mutate(.pred_class = factor(ifelse(.pred_Churn >= df_thresholds$.threshold, "Churn", "Remain"))) %>%
            bind_cols(df_baked_test) %>%
            conf_mat(truth = exited, estimate = .pred_class) %>%
            summary() %>%
            filter(.metric %in% c("accuracy", "sens", "spec")) %>%
            mutate(.cutoff = paste(round(df_thresholds$.threshold * 100),"_perc", sep = ""))
          ) %>%
  spread(.metric, .estimate) %>%
  select(-.estimator) %>%
  mutate(accuracy = round(accuracy, 3),
         sens = round(sens, 3),
         spec = round(spec, 3)) %>%
  kable(align = c('c', 'c', 'c', 'c'))
```

# Konklusion

Som min konklusion kan man sige at tidymodels er en god pakke og framwork til at bygge predictive moddeller. Da modellen er under udvikling vil der komme ændringer og forbedringer undervejs. Hele pakken er meget funktionel og gør at model udvikling er relativ simpel især når man bryder koden ned i mindre bidder.



