---
title: Introduction to Text analysis
author: Lucas Bagge
date: '2020-07-22'
slug: introduction-to-text-analysis
categories:
  - text mining
  - tidymodels
tags:
  - rf
  - caret
  - ggplot2
  - rpart
  - text mining
subtitle: An analysis of sms data
draft: TRUE
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      tidy = "styler", fig.width = 8, fig.height = 5)
library(tidymodels)
library(skimr)
library(tibble)
library(dplyr)
library(tidyr)
library(magrittr)
library(tidyquant)
theme_set(theme_minimal())
```

# Introduction

# What can it be used for?

In todays world there is social media and other online sources with information. Why not exploder these and make analysis. But it is not numbers? Numbers isnÂ´t the only think you can analysis. When you have an resturant you are interting in your [EBITA](https://www.shareholders.dk/investorordbogen/1117/ebita) but you might bee equally interesting in an customer review on trip advisor. The only problems is that if you are owner of a big chain then 1000 of reviews might takes you some time to read though.

It is where Text mining comes in. Text mining is an automatic process that uses natural langauge to extract valuable information from unstructed text to data a model can understand and analysis can draw conclusion on. So from a small to a big business owner one can use text mining to reduce task, get to know ones customer and used data in the most essentiel ways.

There is a lot to do with text mining and different methods and techniques. Because the toolbox is so big it is worth to take a deep drive into what the termology is and get a generel view of the usable practise.

# Methods and Techniques

When using text mining there is different models and techniques that can be used. This is my first shot at using text moding so it is important for me to try to explain the theory I am going to use and the different concepts that are essentiel. 

So the first thing is so do we represent a text as a **data frame**? That is simple (but can gives us problems, which is going to be explain later) just take every word and make it a column. This process is called **Tokenization** and every distinct piece is called a **token**. When this is don we know have a data frame where each row represent a document, each column is a distinct token and each cell is a count of the token for the document. This format is called **document tern matrix**. Here we have a number in the row that count how often one word is being used. This is a very simple (yet very efficient model) and is called **Bag of words** and is often the first model you use. When you start out this is a good standard but there are different of problems you should consider as; casing, punctuation, numbers, every word, symbolds and similiar words.



# Data

# Code 

# Result

# Conclusion
