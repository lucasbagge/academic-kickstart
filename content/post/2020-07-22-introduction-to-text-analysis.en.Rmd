---
title: Introduction to Text analysis
author: Lucas Bagge
date: '2020-07-22'
slug: introduction-to-text-analysis
categories:
  - text mining
  - tidymodels
tags:
  - rf
  - caret
  - ggplot2
  - rpart
  - text mining
subtitle: An analysis of sms data
draft: TRUE
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      tidy = "styler", fig.width = 8, fig.height = 5)
library(tidymodels)
library(skimr)
library(plotly)
library(tibble)
library(dplyr)
library(tidyr)
library(magrittr)
library(tidyquant)
library(ggplot2)
library(e1071)
library(caret)
library(doSNOW)
library(quanteda)
library(irlba)
library(randomForest)
theme_set(theme_minimal())
```

# Introduction

# What can it be used for?

In todays world there is social media and other online sources with information. Why not exploder these and make analysis. But it is not numbers? Numbers isn´t the only think you can analysis. When you have an resturant you are interting in your [EBITA](https://www.shareholders.dk/investorordbogen/1117/ebita) but you might bee equally interesting in an customer review on trip advisor. The only problems is that if you are owner of a big chain then 1000 of reviews might takes you some time to read though.

It is where Text mining comes in. Text mining is an automatic process that uses natural langauge to extract valuable information from unstructed text to data a model can understand and analysis can draw conclusion on. So from a small to a big business owner one can use text mining to reduce task, get to know ones customer and used data in the most essentiel ways.

There is a lot to do with text mining and different methods and techniques. Because the toolbox is so big it is worth to take a deep drive into what the termology is and get a generel view of the usable practise.

# Methods and Techniques

When using text mining there is different models and techniques that can be used. This is my first shot at using text moding so it is important for me to try to explain the theory I am going to use and the different concepts that are essentiel. 

So the first thing is so do we represent a text as a **data frame**? That is simple (but can gives us problems, which is going to be explain later) just take every word and make it a column. This process is called **Tokenization** and every distinct piece is called a **token**. When this is don we know have a data frame where each row represent a document, each column is a distinct token and each cell is a count of the token for the document. This format is called **document tern matrix**. Here we have a number in the rows that count how often one word is being used. This is a very simple (yet very efficient model) and is called **Bag of words** and is often the first model you use. When you start out this is a good standard but there are different of problems you should consider as; casing, punctuation, numbers, every word, symbolds and similiar words.

When you have a large unstructured document it is called **corpus**. It comes from the medicin world (or physiological) where corpus means the whole human body. 

We will later see when we build the DTF that it works but as mentioned it has its problems when the documents becomes very large because then terms will appear frequentyl but is not that important. For solving this issue one can **normalize** and penalize terms that occur frequency across the corpus. 

## TF-IDF

As the first step one need to calculate the **term frequency** that is giving by:

$$
TF(t,d) = \frac{freq(t,d)}{\sum^n_i freq(t_i,d)}
$$
Here *freq* is the count instance of the term, *t*, in the document, *d*. TF is the proportion af the count  *t* in d. This is an incomplete measure of how important a word in a text. It is incomplete because some words as; "is" etc is not neassayc important. Here one can load in some R packages that handles this and it is called **stop words**. In text mining this approch is not best practise because we actuallt don´t know beforehand that some word is important for the text.   This can be seen as normalize the document.

Another approch is to look at a terms **inverse document frequency (idf)**, which decrease the weight of words that is used often and increases the weight of the word that are used often;

$$
idf(t) = log \ (\frac{N}{count(t)})
$$

Here N is the distinct document in the corpus and count(t) is the number of document where the t is appearing.

Combing the two equation we get a tf-idf which empower our document term frequency model from before.

The tf-idf is giving by;

$$
tf-idf=tf(t,d)*idf(t)
$$
So to sum up tf-idf is a powerful techinuqe to enhance the information/signal because we by this method normalize the data and weigth it. 

## n-grams

But there is more to look at for this still simple model. For know we have looked at words as a single value. Words often cannot be seen as one word but between words. This is where **n-grams** in a usefull tools. When applying n-grams we look at the relationships between words and this can give us many insight. Here we get the option to ordering our words in the bags of word model. 

An overview of the names of ngrams is the following:

- unigrams
- bigrams
- trigrams
- 4-grams
- 5-grams

This type of analysis is good to make a correlation analysis between words. One problem is that we gonna expand the size of the matrix. This problem is a huge and very often problem called **curse of dimensionality**

##  Curse of Dimensionality

What this mean is that you are getting to many features. This can lead to several problems:

- If we have more features than observatoin than we might overfit our model - this will give us terrible out of sample performance. 
- **Sparsity** where our data becomes minor because of the growth of dimension.

A way of solving this issue is by using more advanced technique. There a multiple techniques but I will talked about **latent semantic analysis**

## Latent Semantic analysis

By its core LSA are using the thought befind a **Vector Space Model**. By this we work with tect as vectors of numbers and allow us to work with document geometrically. When we can think about documents geomtrically gives us advantages as seeing correlation between documents

måske billede

We can analysis this relationship by using the **dot product** where we see how different word/documents/terms are related to each other. 

By understanding the fundamental theory of the VSM model we can expand it to the LSA model. LSM are using this framwork by leveraging a **singular value decomposition (SVD)** where you factorize a term document to extract similarities in the terms. 

$$
SVD \ of \ X=t(x)=U\sum V^T
$$

LSA uses **singular value matrix** as can bee seen from the formular above. This is a factorization tecnique and as mention resample how we calculate the dot product of the different entities.   

For understnading the ceoncept we first have to look at **eigenvalues** which is a diagonally symmetric matrix (meaning that if you flip it than it would look the same). If the matrix is diagonal than it the off diagonal cells are zero. For any matrix of this form there will be are **orthonomal basis** (meaning normalized vectors) and here we get eigenvectors that only extend the original matrix. 

SVD is an extension and includes arbritary matrices and therefore need another vector set. It is giving by the matrices we have calculated in our tf-idf matrix. 

So the overall technique is SVM and LSA describes the application to a particular kind of matrix, mostly a textual data and in our example tf-idf matrix. The idea is to create a **semantic** space where word and documents can be compared. Before we have a finished data we need to project the the new data meaning we need to transform it. It should be mention that it is approximation meaning that we don´t optaion the exact value which gives a problem with liability. 

When we build on the lsa and use this to evaluate a model we need some guidlines to evaluate the models. Here there are different matrices that can be used and I will go though some of them that I rely on. 

## Evaluation matrices

The generel framwork we work outfrom when looking at a classification problem is by looking at the **confusion matrix (cm)**. 

billede

The first metrices we gonna use is the **Accuracy** that tells us what percentage of the predictions were correct.

billede

Another important metrices we can extract from cf is **sensitivity**. It measires what percentage of the ham messages were correctly predicted.

billede

The last metric the post will put focus on is **specificity**. Here we look at what percentage of the spam message were correctly preducted.

Know the techniques and method has been discribed on what is going to be use in the modelling and coding phase. Before going to that parth we need to look and talk about the 

# Data

This post is going to use sms data from the popular site [kaggle](https://www.kaggle.com/ishansoni/sms-spam-collection-dataset)

This is a collection of sms spam that have been used to analysis of being ham or spam. 

The flle contain one message per line. Each line is compoased by twp columns; v1 contain the label (ham or spam) and v2 contain the text message. 

# Analysis

The first thing is loading in the data.

```{r}
spam.raw <- read.csv("../../../tutorials-master-906d4dbc73b47d7af6f725c415896d8fd20b048c/Introduction to Text Analytics with R/spam.csv", 
                     stringsAsFactors = FALSE, 
                     fileEncoding = "UTF-16")
```

The data need to be clean and this is one of the most important task in text mining. Here we also make the Label column to a factor. This ia our categorical variable wee want to build our model around.

```{r}
spam.raw <- spam.raw %>% 
  select(v1,v2) %>% 
  rename(Label = v1,
         Text = v2) %>% 
  mutate(Label = as.factor(Label))
```

Check if there is some missing values in the data.

```{r}
length(which(!complete.cases(spam.raw)))
```
There is nothing missing in the data

## Explanatory data analysis

# First, let's take a look at distibution of the class labels (i.e., ham vs. spam).
 The first thing is to look at the distribution of the class label. Here I calculate the proportion of the different classes.

Another way of showing the difference is by a visualization with a pie charts.


```{r}
mycols <- c("#0073C2FF", "#EFC000FF")
library(highcharter)
p %>% 
  hchart(type = "pie", hcaes(x = Label, y = prop)) %>% 
  hc_title(text = "Marshall’s Favorite bars",
           align = "center",
           style = list(fontWeight = "bold", fontSize = "30px")) %>% 
  hc_tooltip(enabled = T) %>% 
  hc_subtitle(text = "In Percentage",
              align = "center",
              style = list(fontWeight = "bold")) %>% 
  hc_add_theme(hc_theme_economist()) %>% 
  hc_credits(enabled = TRUE, text = "Data source:HIMYM")

```

As a next step we create (or feature enginering) a new variable called `TextLength` that measue the length of the sms text. This is visualized with a histrogram so we get familiar with the distribution of ham and spam.

```{r}
spam.raw <- spam.raw %>% 
  mutate(TextLength = nchar(Text))

p <- ggplot(spam.raw, aes(x = TextLength, fill = Label)) +
  geom_histogram(binwidth = 5) +
  labs(y = "Text Count", x = "Length of Text",
       title = "Distribution of Text Lengths with Class Labels")

ggplotly(p)
```

# Modelling

## Data split

As a first step in the modelling phase we want to split the data into a traning and test set. This should be extended to include a validation set because this is a more correct way in a real project. 

We saw from our pie chart that we have an imblance in our data. To solve this issue we use tidymodels/caret to correct this so we have the right proportion.

```{r}
set.seed(32984)
indexes <- createDataPartition(spam.raw$Label,
                               times = 1,
                               p = 0.7, 
                               list = FALSE)
train <- spam.raw[indexes,]
test <- spam.raw[-indexes,]
```

The split size 70%/30% is used ofent in the litteratur. Let us verify that we have the right proportions in the new data.

```{r}
prop.table(table(train$Label))
prop.table(table(test$Label))

```


## Furtere exploration, pre-processing and wrangling

For text analytics there is often a lot wrong with the text string. Some of thease errors are url and signs as ?+- etc.

```{r}
train$Text[518]
```

```{r}
url_test <-  
  train %>% 
  filter(grepl("www", Text))

url_test %>% 
  head() %>% 
  kable()
```

There are many packges to solve such issues. I have gotten familiar with `quanteda` which has many good functions to work with text data.

```{r}
train.tokens <- tokens(train$Text, 
                       what = "word", 
                       remove_numbers = TRUE,
                       remove_punct = TRUE,
                       remove_symbols = TRUE, 
                       remove_hyphens = TRUE)

```

```{r}
train.tokens[[518]]
```
We see that with the function above we split the text up for every words.

The first thing is to lower case every tokens.
```{r}
train.tokens <- tokens_tolower(train.tokens)
train.tokens[[518]]

```
In the quanteda packages there is a build in stop-words list for English which can be used.

```{r}
train.tokens <- tokens_select(train.tokens, 
                              stopwords(), 
                              selection = "remove")
train.tokens[[518]]
```
Here we can see that our sentence has been reduced because it included what is considered stop-words. 

The next step is to reduced word to the stems. This process is called **stemming** - skal op i vores metodik afsnit.

```{r}
train.tokens <- tokens_wordstem(train.tokens, 
                                language = "english")
train.tokens[[518]]

```
Know we can create our first **bag of words model**

```{r}
train.tokens.dfm <- dfm(train.tokens, 
                        tolower = FALSE)

```
To inspect the model we need to transform it to a matrix.

```{r}
train.tokens.matrix <- as.matrix(train.tokens.dfm)
dim(train.tokens.matrix)
```

A second step I consider as best practices is to leverage cross validation in our basis of the modeling process. By using CV we create estimates of how well our model will do in production on new data. The downside is it takes time.

The feature data is setup with the labels.

```{r}
train.tokens.df <- cbind(Label = train$Label,
                         data.frame(train.tokens.dfm))
```

As an additional step ther oftens need more pre processing.

```{r}
names(train.tokens.df)[c(146, 148, 235, 238)]
```
```{r}
names(train.tokens.df) <- make.names(names(train.tokens.df))
```

Make the stratified folds for 10 fold cross valudation repated 3 times therefore we have 30 random stratified samples.

```{r}
set.seed(48743)
cv.folds <- createMultiFolds(train$Label, k = 10, times = 3)

cv.cntrl <- trainControl(method = "repeatedcv",
                         number = 10,
                         repeats = 3,
                         index = cv.folds)

```

Our data frame is big and therefore cv will take some time to run. To ct down the execution time we use `doSNOW` for allowing a multi-core traning in parallel.

Here it is important to consider the strengt of you computer. My office computer uses 12 logical cores so it should be change depending on the computer.

```{r}
# Time the code execution
start.time <- Sys.time()

# Create a cluster to work on 10 logical cores.
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)

# As our data is non-trivial in size at this point, use a single decision
# tree alogrithm as our first model. We will graduate to using more 
# powerful algorithms later when we perform feature extraction to shrink
# the size of our data.
rpart.cv.1 <- train(Label ~ ., data = train.tokens.df, method = "rpart", 
                    trControl = cv.cntrl, tuneLength = 7)

# Processing is done, stop cluster.
stopCluster(cl)

# Total time of execution on workstation was approximately 4 minutes. 
total.time <- Sys.time() - start.time
total.time

```



# Check out our results.
```{r}
rpart.cv.1
```
From the output above we see that our tf-idf model is good and gives us a good score when looking at the **Accuracy**. This method have three strenghs: 


1 - The TF calculation accounts for the fact that longer 
    documents will have higher individual term counts. Applying
    TF normalizes all documents in the corpus to be length 
    independent.
2 - The IDF calculation accounts for the frequency of term
    appearance in all documents in the corpus. The intuition 
    being that a term that appears in every document has no
    predictive power.
3 - The multiplication of TF by IDF for each cell in the matrix
    allows for weighting of #1 and #2 for each cell in the matrix.

```{r}
# Our function for calculating relative term frequency (TF)
term.frequency <- function(row) {
  row / sum(row)
}

```

```{r}
# Our function for calculating inverse document frequency (IDF)
inverse.doc.freq <- function(col) {
  corpus.size <- length(col)
  doc.count <- length(which(col > 0))

  log10(corpus.size / doc.count)
}

```

```{r}
# Our function for calculating TF-IDF.
tf.idf <- function(x, idf) {
  x * idf
}

```

First step, normalize all documents via TF.
```{r}
train.tokens.df <- apply(train.tokens.matrix, 1, term.frequency)
dim(train.tokens.df)
```
```{r}
# Second step, calculate the IDF vector that we will use - both
# for training data and for test data!
train.tokens.idf <- apply(train.tokens.matrix, 2, inverse.doc.freq)
str(train.tokens.idf)
```

```{r}
# Lastly, calculate TF-IDF for our training corpus.
train.tokens.tfidf <-  apply(train.tokens.df, 2, tf.idf, idf = train.tokens.idf)
dim(train.tokens.tfidf)
```
```{r}
# Transpose the matrix
train.tokens.tfidf <- t(train.tokens.tfidf)
dim(train.tokens.tfidf)
```

```{r}
incomplete.cases <- which(!complete.cases(train.tokens.tfidf))
train$Text[incomplete.cases]
```

```{r}
# Fix incomplete cases
train.tokens.tfidf[incomplete.cases,] <- rep(0.0, ncol(train.tokens.tfidf))
dim(train.tokens.tfidf)
sum(which(!complete.cases(train.tokens.tfidf)))
```

```{r}
# Make a clean data frame using the same process as before.
train.tokens.tfidf.df <- cbind(Label = train$Label, data.frame(train.tokens.tfidf))
names(train.tokens.tfidf.df) <- make.names(names(train.tokens.tfidf.df))
```

```{r}
# Time the code execution
start.time <- Sys.time()

# Create a cluster to work on 10 logical cores.
cl <- makeCluster(5, type = "SOCK")
registerDoSNOW(cl)

# As our data is non-trivial in size at this point, use a single decision
# tree alogrithm as our first model. We will graduate to using more 
# powerful algorithms later when we perform feature extraction to shrink
# the size of our data.
rpart.cv.2 <- train(Label ~ ., data = train.tokens.tfidf.df, method = "rpart", 
                    trControl = cv.cntrl, tuneLength = 7)

# Processing is done, stop cluster.
stopCluster(cl)

# Total time of execution on workstation was 
total.time <- Sys.time() - start.time
total.time
```
```{r}
# Check out our results.
rpart.cv.2
```

## N-grams

As mention if we include bigrams (i.e., double terms) we often see an increse in performance (e.g., accuracy). This is added to our training data and the **TF-IDF** is expanded and we will see if our accuracy is improved.

```{r}
# Add bigrams to our feature matrix.
train.tokens <- tokens_ngrams(train.tokens, n = 1:2)
train.tokens[[357]]

# Transform to dfm and then a matrix.
train.tokens.dfm <- dfm(train.tokens, tolower = FALSE)
train.tokens.matrix <- as.matrix(train.tokens.dfm)
train.tokens.dfm

# Normalize all documents via TF.
train.tokens.df <- apply(train.tokens.matrix, 1, term.frequency)

# Calculate the IDF vector that we will use for training and test data!
train.tokens.idf <- apply(train.tokens.matrix, 2, inverse.doc.freq)

# Calculate TF-IDF for our training corpus 
train.tokens.tfidf <-  apply(train.tokens.df, 2, tf.idf, 
                             idf = train.tokens.idf)

# Transpose the matrix
train.tokens.tfidf <- t(train.tokens.tfidf)

# Fix incomplete cases
incomplete.cases <- which(!complete.cases(train.tokens.tfidf))
train.tokens.tfidf[incomplete.cases,] <- rep(0.0, ncol(train.tokens.tfidf))

# Make a clean data frame.
train.tokens.tfidf.df <- cbind(Label = train$Label, data.frame(train.tokens.tfidf))
names(train.tokens.tfidf.df) <- make.names(names(train.tokens.tfidf.df))

# Clean up unused objects in memory.
gc()
```




#
# NOTE - The following code requires the use of command-line R to execute
#        due to the large number of features (i.e., columns) in the matrix.
#        Please consult the following link for more details if you wish
#        to run the code yourself:
#
#        https://stackoverflow.com/questions/28728774/how-to-set-max-ppsize-in-r
#
#        Also note that running the following code required approximately
#        38GB of RAM and more than 4.5 hours to execute on a 10-core 
#        workstation!
#


# Time the code execution
# start.time <- Sys.time()

# Leverage single decision trees to evaluate if adding bigrams improves the 
# the effectiveness of the model.
# rpart.cv.3 <- train(Label ~ ., data = train.tokens.tfidf.df, method = "rpart", 
#                     trControl = cv.cntrl, tuneLength = 7)

# Total time of execution on workstation was
# total.time <- Sys.time() - start.time
# total.time

# Check out our results.
# rpart.cv.3

#
# The results of the above processing show a slight decline in rpart 
# effectiveness with a 10-fold CV repeated 3 times accuracy of 0.9457.
# As we will discuss later, while the addition of bigrams appears to 
# negatively impact a single decision tree, it helps with the mighty
# random forest!
#




# We'll leverage the irlba package for our singular value 
# decomposition (SVD). The irlba package allows us to specify
# the number of the most important singular vectors we wish to
# calculate and retain for features.
library(irlba)


# Time the code execution
start.time <- Sys.time()

# Perform SVD. Specifically, reduce dimensionality down to 300 columns
# for our latent semantic analysis (LSA).
train.irlba <- irlba(t(train.tokens.tfidf), nv = 300, maxit = 600)

# Total time of execution on workstation was 
total.time <- Sys.time() - start.time
total.time


# Take a look at the new feature data up close.
View(train.irlba$v)


# As with TF-IDF, we will need to project new data (e.g., the test data)
# into the SVD semantic space. The following code illustrates how to do
# this using a row of the training data that has already been transformed
# by TF-IDF, per the mathematics illustrated in the slides.
#
#
sigma.inverse <- 1 / train.irlba$d
u.transpose <- t(train.irlba$u)
document <- train.tokens.tfidf[1,]
document.hat <- sigma.inverse * u.transpose %*% document

# Look at the first 10 components of projected document and the corresponding
# row in our document semantic space (i.e., the V matrix)
document.hat[1:10]
train.irlba$v[1, 1:10]



#
# Create new feature data frame using our document semantic space of 300
# features (i.e., the V matrix from our SVD).
#
train.svd <- data.frame(Label = train$Label, train.irlba$v)


# Create a cluster to work on 10 logical cores.
cl <- makeCluster(10, type = "SOCK")
registerDoSNOW(cl)

# Time the code execution
start.time <- Sys.time()

# This will be the last run using single decision trees. With a much smaller
# feature matrix we can now use more powerful methods like the mighty Random
# Forest from now on!
rpart.cv.4 <- train(Label ~ ., data = train.svd, method = "rpart", 
                    trControl = cv.cntrl, tuneLength = 7)

# Processing is done, stop cluster.
stopCluster(cl)

# Total time of execution on workstation was 
total.time <- Sys.time() - start.time
total.time

# Check out our results.
rpart.cv.4




#
# NOTE - The following code takes a long time to run. Here's the math.
#        We are performing 10-fold CV repeated 3 times. That means we
#        need to build 30 models. We are also asking caret to try 7 
#        different values of the mtry parameter. Next up by default
#        a mighty random forest leverages 500 trees. Lastly, caret will
#        build 1 final model at the end of the process with the best 
#        mtry value over all the training data. Here's the number of 
#        tree we're building:
#
#             (10 * 3 * 7 * 500) + 500 = 105,500 trees!
#
# On a workstation using 10 cores the following code took 28 minutes 
# to execute.
#


# Create a cluster to work on 10 logical cores.
# cl <- makeCluster(10, type = "SOCK")
# registerDoSNOW(cl)

# Time the code execution
# start.time <- Sys.time()

# We have reduced the dimensionality of our data using SVD. Also, the 
# application of SVD allows us to use LSA to simultaneously increase the
# information density of each feature. To prove this out, leverage a 
# mighty Random Forest with the default of 500 trees. We'll also ask
# caret to try 7 different values of mtry to find the mtry value that 
# gives the best result!
# rf.cv.1 <- train(Label ~ ., data = train.svd, method = "rf", 
#                 trControl = cv.cntrl, tuneLength = 7)

# Processing is done, stop cluster.
# stopCluster(cl)

# Total time of execution on workstation was 
# total.time <- Sys.time() - start.time
# total.time


# Load processing results from disk!
load("rf.cv.1.RData")

# Check out our results.
rf.cv.1

# Let's drill-down on the results.
confusionMatrix(train.svd$Label, rf.cv.1$finalModel$predicted)





# OK, now let's add in the feature we engineered previously for SMS 
# text length to see if it improves things.
train.svd$TextLength <- train$TextLength


# Create a cluster to work on 10 logical cores.
# cl <- makeCluster(10, type = "SOCK")
# registerDoSNOW(cl)

# Time the code execution
# start.time <- Sys.time()

# Re-run the training process with the additional feature.
# rf.cv.2 <- train(Label ~ ., data = train.svd, method = "rf",
#                 trControl = cv.cntrl, tuneLength = 7, 
#                 importance = TRUE)

# Processing is done, stop cluster.
# stopCluster(cl)

# Total time of execution on workstation was 
# total.time <- Sys.time() - start.time
# total.time

# Load results from disk.
load("rf.cv.2.RData")

# Check the results.
rf.cv.2

# Drill-down on the results.
confusionMatrix(train.svd$Label, rf.cv.2$finalModel$predicted)

# How important was the new feature?
library(randomForest)
varImpPlot(rf.cv.1$finalModel)
varImpPlot(rf.cv.2$finalModel)




# Turns out that our TextLength feature is very predictive and pushed our
# overall accuracy over the training data to 97.1%. We can also use the
# power of cosine similarity to engineer a feature for calculating, on 
# average, how alike each SMS text message is to all of the spam messages.
# The hypothesis here is that our use of bigrams, tf-idf, and LSA have 
# produced a representation where ham SMS messages should have low cosine
# similarities with spam SMS messages and vice versa.

# Use the lsa package's cosine function for our calculations.
#install.packages("lsa")
library(lsa)

train.similarities <- cosine(t(as.matrix(train.svd[, -c(1, ncol(train.svd))])))


# Next up - take each SMS text message and find what the mean cosine 
# similarity is for each SMS text mean with each of the spam SMS messages.
# Per our hypothesis, ham SMS text messages should have relatively low
# cosine similarities with spam messages and vice versa!
spam.indexes <- which(train$Label == "spam")

train.svd$SpamSimilarity <- rep(0.0, nrow(train.svd))
for(i in 1:nrow(train.svd)) {
  train.svd$SpamSimilarity[i] <- mean(train.similarities[i, spam.indexes])  
}


# As always, let's visualize our results using the mighty ggplot2
ggplot(train.svd, aes(x = SpamSimilarity, fill = Label)) +
  theme_bw() +
  geom_histogram(binwidth = 0.05) +
  labs(y = "Message Count",
       x = "Mean Spam Message Cosine Similarity",
       title = "Distribution of Ham vs. Spam Using Spam Cosine Similarity")


# Per our analysis of mighty random forest results, we are interested in 
# in features that can raise model performance with respect to sensitivity.
# Perform another CV process using the new spam cosine similarity feature.

# Create a cluster to work on 10 logical cores.
# cl <- makeCluster(10, type = "SOCK")
# registerDoSNOW(cl)

# Time the code execution
# start.time <- Sys.time()
 
# Re-run the training process with the additional feature.
# set.seed(932847)
# rf.cv.3 <- train(Label ~ ., data = train.svd, method = "rf",
#                 trControl = cv.cntrl, tuneLength = 7,
#                 importance = TRUE)

# Processing is done, stop cluster.
# stopCluster(cl)

# Total time of execution on workstation was 
# total.time <- Sys.time() - start.time
# total.time


# Load results from disk.
load("rf.cv.3.RData")

# Check the results.
rf.cv.3

# Drill-down on the results.
confusionMatrix(train.svd$Label, rf.cv.3$finalModel$predicted)

# How important was this feature?
library(randomForest)
varImpPlot(rf.cv.3$finalModel)




# We've built what appears to be an effective predictive model. Time to verify
# using the test holdout data we set aside at the beginning of the project.
# First stage of this verification is running the test data through our pre-
# processing pipeline of:
#      1 - Tokenization
#      2 - Lower casing
#      3 - Stopword removal
#      4 - Stemming
#      5 - Adding bigrams
#      6 - Transform to dfm
#      7 - Ensure test dfm has same features as train dfm

# Tokenization.
test.tokens <- tokens(test$Text, what = "word", 
                      remove_numbers = TRUE, remove_punct = TRUE,
                      remove_symbols = TRUE, remove_hyphens = TRUE)

# Lower case the tokens.
test.tokens <- tokens_tolower(test.tokens)

# Stopword removal.
test.tokens <- tokens_select(test.tokens, stopwords(), 
                             selection = "remove")

# Stemming.
test.tokens <- tokens_wordstem(test.tokens, language = "english")

# Add bigrams.
test.tokens <- tokens_ngrams(test.tokens, n = 1:2)

# Convert n-grams to quanteda document-term frequency matrix.
test.tokens.dfm <- dfm(test.tokens, tolower = FALSE)

# Explore the train and test quanteda dfm objects.
train.tokens.dfm
test.tokens.dfm

# Ensure the test dfm has the same n-grams as the training dfm.
#
# NOTE - In production we should expect that new text messages will 
#        contain n-grams that did not exist in the original training
#        data. As such, we need to strip those n-grams out.
#
test.tokens.dfm <- dfm_select(test.tokens.dfm, pattern = train.tokens.dfm,
                              selection = "keep")
test.tokens.matrix <- as.matrix(test.tokens.dfm)
test.tokens.dfm




# With the raw test features in place next up is the projecting the term
# counts for the unigrams into the same TF-IDF vector space as our training
# data. The high level process is as follows:
#      1 - Normalize each document (i.e, each row)
#      2 - Perform IDF multiplication using training IDF values

# Normalize all documents via TF.
test.tokens.df <- apply(test.tokens.matrix, 1, term.frequency)
str(test.tokens.df)

# Lastly, calculate TF-IDF for our training corpus.
test.tokens.tfidf <-  apply(test.tokens.df, 2, tf.idf, idf = train.tokens.idf)
dim(test.tokens.tfidf)
View(test.tokens.tfidf[1:25, 1:25])

# Transpose the matrix
test.tokens.tfidf <- t(test.tokens.tfidf)

# Fix incomplete cases
summary(test.tokens.tfidf[1,])
test.tokens.tfidf[is.na(test.tokens.tfidf)] <- 0.0
summary(test.tokens.tfidf[1,])




# With the test data projected into the TF-IDF vector space of the training
# data we can now to the final projection into the training LSA semantic
# space (i.e. the SVD matrix factorization).
test.svd.raw <- t(sigma.inverse * u.transpose %*% t(test.tokens.tfidf))


# Lastly, we can now build the test data frame to feed into our trained
# machine learning model for predictions. First up, add Label and TextLength.
test.svd <- data.frame(Label = test$Label, test.svd.raw, 
                       TextLength = test$TextLength)


# Next step, calculate SpamSimilarity for all the test documents. First up, 
# create a spam similarity matrix.
test.similarities <- rbind(test.svd.raw, train.irlba$v[spam.indexes,])
test.similarities <- cosine(t(test.similarities))


#
# NOTE - The following code was updated post-video recoding due to a bug.
#
test.svd$SpamSimilarity <- rep(0.0, nrow(test.svd))
spam.cols <- (nrow(test.svd) + 1):ncol(test.similarities)
for(i in 1:nrow(test.svd)) {
  # The following line has the bug fix.
  test.svd$SpamSimilarity[i] <- mean(test.similarities[i, spam.cols])  
}


# Some SMS text messages become empty as a result of stopword and special 
# character removal. This results in spam similarity measures of 0. Correct.
# This code as added post-video as part of the bug fix.
test.svd$SpamSimilarity[!is.finite(test.svd$SpamSimilarity)] <- 0


# Now we can make predictions on the test data set using our trained mighty 
# random forest.
preds <- predict(rf.cv.3, test.svd)


# Drill-in on results
confusionMatrix(preds, test.svd$Label)




# The definition of overfitting is doing far better on the training data as
# evidenced by CV than doing on a hold-out dataset (i.e., our test dataset).
# One potential explantion of this overfitting is the use of the spam similarity
# feature. The hypothesis here is that spam features (i.e., text content) varies
# highly, espeically over time. As such, our average spam cosine similarity 
# is likely to overfit to the training data. To combat this, let's rebuild a
# mighty random forest without the spam similarity feature.
train.svd$SpamSimilarity <- NULL
test.svd$SpamSimilarity <- NULL


# Create a cluster to work on 10 logical cores.
# cl <- makeCluster(10, type = "SOCK")
# registerDoSNOW(cl)

# Time the code execution
# start.time <- Sys.time()

# Re-run the training process with the additional feature.
# set.seed(254812)
# rf.cv.4 <- train(Label ~ ., data = train.svd, method = "rf",
#                  trControl = cv.cntrl, tuneLength = 7,
#                  importance = TRUE)

# Processing is done, stop cluster.
# stopCluster(cl)

# Total time of execution on workstation was
# total.time <- Sys.time() - start.time
# total.time


# Load results from disk.
load("C:/Users/LUCBA/Projects/tutorials-master-906d4dbc73b47d7af6f725c415896d8fd20b048c/Introduction to Text Analytics with R/rf.cv.4.RData")

# Make predictions and drill-in on the results
preds <- predict(rf.cv.4, test.svd)
confusionMatrix(preds, test.svd$Label)


# Result

# Conclusion

# Litteratur

- https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e?gi=eb64567f0820
