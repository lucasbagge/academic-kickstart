---
title: Introduction to machine learning - KNN
author: Lucas Bagge
date: '2020-11-18'
slug: introduction-to-machine-learning-knn
categories: []
tags:
  - ML
  - KNN
subtitle: ''
summary: ''
authors: []
lastmod: '2020-05-04T22:31:29+02:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: [Machine learning series]
draft: true
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      tidy = "styler", fig.width = 8, fig.height = 5)
library(tidyverse)
library(tidymodels)
library(tidyquant)
theme_set(theme_minimal())
```

# Introduction

I have decided on making a series of blog post about Machine Learning and the different algoritme.

In this post I will go though **K-Nearest Neighbors (Knn)** which can be used for classification or prediction. The algoritme relies on similar record in the data and we use this to derive a classification for new records.

I will explain how similarity is detemrined, how the number of neighbors is chosen, and how
a model is being computed. furthere I will discuss the advantages and weaknesses of the
method.

# Knn classifier

The idea behind knn algoritme is to identify *k* record in the data that are
similar to a new records that one whish to classify. These neighbor will be
used to classify a new record into a class. So we are looking for some similarity
in the data.

The algoritme dosen make assumptions on the relationship between the classes, $Y$
and the predictors $X_1,X_2,...,X_n$. It is a nonparametric meaning it
does not involve estiamtion of parameters in a function such as the linear
regression. 

An important aspect is how to measure the distance! A popular measure is 
**Euclidean distance** which is given by:

$$
\sqrt{(x_1-u_1)^2 + (x_2-u_2)^2 + ... + (x_p-u_p)^2}
$$
When the distance has been calculated we need to assign a rule for the classes
based on the neighbors. The simplest case is k=1, where we look at the closes
neighbor and classify the record as the same as the nearest.  

## Chosing k

It could be tempting to choose k=1 all the same but here one risk to fit the model
to noise. On the other side if k is to high we might miss out on capturing
patteren in the data. So we need to balance it and it is complely based on
the data. We should choose k with the best classification performance. 

## Advantages and shortcomings

The main advantages for the Knn algoritme is how simple it is to understand.

The big shortcomings is that the number of records can increase exponentially
with the numner of predictors. It is because the expected distance to
the nearst neighbor goes up exponentially with the number of predictors. It
is way one seek to reduce the nummber of predictors. 

# Model

For illustration purpose I want to implement the model in the `tidymodels` 
framwork. 

## Data

```{r}
data(bivariate)
glimpse(bivariate_train)
```

```{r}
df  <- tq_get("ORSTED.CO", get = "stock.prices", from = " 1990-01-01")
df %>% glimpse()
```

We will look at the closing prices as an idicator and create a new column called `Type` which
indicate weather the stock went up or down. 

```{r}
df <- df %>% 
  select(date, close) %>% 
  mutate(type = case_when(
    lag(close) < close ~ "Up",
    lag(close) > close ~ "Down",
    TRUE ~ "Other"
  )) %>% 
  filter(type != "Other") %>% 
  mutate(across(where(is.character), as.factor))
df %>% glimpse()
```

## EDA

We need to explore how our data look likes. Because we are having a stock
the data is gonna be very volatile.

```{r}
df %>% 
  ggplot(aes(x = date, y = close)) +
  geom_line()
```
From the above plot it can be seen that that the stock has gone up.

## Split data

In the modelling path we need to split our data:

```{r}
set.seed(4595)
split <- initial_split(df,
                       prop = 0.75,
                       strata = type)

df_train <- training(split)

df_test <- testing(split)
```


## Cross validation

When of the thing we need as explain earlier is to choose the best *k*. For
doing that we need to use a method called **cross validation**. What it
basically mean is that we are splitting our data up it smaller paths for traning
our data. 

```{r}
set.seed(1989)
folds <-  vfold_cv(df_train, v = 5, strata = 'type') 
```

## Recipe

The cross validation is gonna be used later. Now we need to define our `recipe`
which is how we are gonna preprocess our data. For this recipe we are gonna
normalize the data and center it. 

```{r}
df_rec <- 
  recipe(type ~ close, data = df_train) %>% 
    # if you are planning to normalize your numerical values
  step_normalize(all_numeric()) %>%
  # if you are planning to knn-ly fill the missing values for categorical type
  step_center(all_predictors()) %>% 
  prep()

# For testing when we arrive at a final model: 
test_normalized <- bake(df_rec, new_data = df_test, all_predictors())
```

## Model

The model we are gonna build is ofcourse a knn models. As mention we don´t know
what k should be therefore wee need to tune it. This mean we are gonna 
train our model and then we can choose the best k that gives the best results.

```{r}
knn_mod <- 
  nearest_neighbor(
    neighbors = tune()) %>%  # ved 2 bruger vi Euclidean distance
  set_engine("kknn") %>% 
  set_mode("classification")  
```

The tuning of our parameter is an important step and it is gonna give us
a best guess on what we should consider as the optimal k.

```{r}
gridvals <- 
  tibble(neighbors = seq(1,200))

best_k <- 
  workflow() %>%
  add_recipe(df_rec) %>%
  add_model(knn_mod) %>%
  tune_grid(folds, grid = gridvals) %>% 
  collect_metrics() %>% 
  filter(.metric == "roc_auc",
         mean == min(mean)) %>% 
  pull(neighbors)

```

From the above result we have use the grid function to make optimization on
the resampling models to find which optimal neighbour we should choose. We
get the best result when we have 17 neigbours so that is what we are going with.

So the next next doing the same thing know with the optimal number of neighbors.

```{r}
knn_tun_spec <-
  nearest_neighbor(neighbors = best_k) %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_mult_fit <- 
  workflow() %>%
  add_recipe(df_rec) %>%
  add_model(knn_tun_spec) %>%
  fit(data = df_train)

knn_mult_preds <-
  knn_mult_fit %>%
  predict(df_test) %>%
  bind_cols(df_test)

knn_mult_mets <-
  metrics(knn_mult_preds, truth = type, estimate = .pred_class) 
knn_mult_mets
```

Our model doesn't´t do the best job of prediction our test set. It is understandable
considering the nature of stocks.

# Conclusions

Knn is a simple at good method in making classification. It has its shortcomings
but the simplicity is a plus.

We have look at the Ørsted stock and our models didn't do a great job in 
classify if it should go up or down.

The results is not surprising given the volatility in the stock.

# Links

- for getting stock data with tidy using [`tidyquant`](https://cran.r-project.org/web/packages/tidyquant/vignettes/TQ01-core-functions-in-tidyquant.html)
