)
folded <-
folds %>%
mutate(
recipes = train_test_split %>%
# Prepper is a wrapper for `prep()` which handles `split` objects
map(prepper, recipe = tree_rec),
test_data = train_test_split %>% map(analysis),
rf_fits =
map2(
recipes,
test_data,
~ fit(
rf_mod,
formula(.x),
data = bake(object = .x, newdata = .y),
engine = "randomForest"
)
)
)
recipes
test_data
folded <-
folds %>%
mutate(
recipes = train_test_split)
prepper
folded <-
folds %>%
mutate(
recipes = train_test_split %>%
map(prepper, recipe = tree_rec))
tree_rec
split <- initial_split(telco, prop = 0.8)
train_data <- training(split)
test_data <- testing(split)
rec <- train_tbl %>%
recipe(Churn ~ .,) %>%
step_string2factor(all_nominal(), -all_outcomes()) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
step_center(all_numeric()) %>%
step_scale(all_numeric())
prepped <-
rec %>%
prep(retain = TRUE)
# Baking
train <-
prepped %>%
juice()
test <-
prepped %>%
bake(new_data = test_data)
## Model specification
rf_mod <-
rand_forest(
mode = "classification",
trees = 200
)
## Fitting
rf_fit <-
fit(
object = rf_mod,
formula = formula(prepped),
data = train,
engine = "randomForest"
)
## Predicting!
results <-
tibble(
actual = test$Churn,
predicted = parsnip::predict_class.model_fit(rf_fit, test)
)
## Assessment -- test error
metrics(results, truth = actual, estimate = predicted) %>%
knitr::kable()
conf_mat(results, truth = actual, estimate = predicted)[[1]] %>%
as_tibble() %>%
ggplot(aes(Prediction, Truth, alpha = n)) +
geom_tile(fill = "blue", show.legend = FALSE) +
geom_text(aes(label = n), color = "white", alpha = 1, size = 8) +
theme_minimal() +
labs(
title = "Confusion matrix"
)
library(purrr)
## 10-fold cross validation
folds <- vfold_cv(telco, v = 10)
folded <-
folds %>%
mutate(
recipes = train_test_split %>%
map(prepper, recipe = tree_rec))
conf_mat(results, truth = actual, estimate = predicted)[[1]] %>%
as_tibble() %>%
ggplot(aes(Prediction, Truth, alpha = n)) +
geom_tile(fill = "blue", show.legend = FALSE) +
geom_text(aes(label = n), color = "white", alpha = 1, size = 8) +
theme_minimal() +
labs(
title = "Confusion matrix"
)
## 10-fold cross validation
folds <- vfold_cv(telco, v = 10)
splits
folded <-
folds %>%
mutate(
recipes = splits %>%
# Prepper is a wrapper for `prep()` which handles `split` objects
map(prepper, recipe = rec),
test_data = splits %>% map(analysis),
rf_fits =
map2(
recipes,
test_data,
~ fit(
rf_mod,
formula(.x),
data = bake(object = .x, newdata = .y),
engine = "randomForest"
)
)
)
folded <-
folds %>%
mutate(
recipes = splits %>%
# Prepper is a wrapper for `prep()` which handles `split` objects
map(prepper, recipe = rec),
test_data = splits %>% map(analysis),
rf_fits =
map2(
recipes,
test_data,
~ fit(
rf_mod,
formula(.x),
data = bake(object = .x, new_data = .y),
engine = "randomForest"
)
)
)
## Predict
predict_rf <- function(split, rec, model) {
test <- bake(rec, assessment(split))
tibble(
actual = test$diagnosis,
predicted = predict_class(model, test)
)
}
predictions <-
folded %>%
mutate(
pred =
list(
splits,
recipes,
rf_fits
) %>%
pmap(predict_rf)
)
tibble(
actual = test$Churn,
predicted = predict_class.model_fit(model, test)
)
## Predict
predict_rf <- function(split, rec, model) {
test <- bake(rec, assessment(split))
tibble(
actual = test$Churn,
predicted = predict_class.model_fit(model, test)
)
}
predictions <-
folded %>%
mutate(
pred =
list(
splits,
recipes,
rf_fits
) %>%
pmap(predict_rf)
)
## Evaluate
eval <-
predictions %>%
mutate(
metrics = pred %>% map(~ metrics(., truth = actual, estimate = predicted))
) %>%
select(metrics) %>%
unnest(metrics)
eval %>% knitr::kable()
eval %>%
summarise_at(vars(accuracy), funs(mean, sd)) %>%
knitr::kable()
eval %>%
filter(eval$.metric == "accuracy")
eval %>%
filter(eval$.metric == "accuracy") %>%
summarise_at(vars(accuracy), funs(mean, sd)) %>%
knitr::kable()
eval %>%
filter(eval$.metric == "accuracy") %>%
summarise_at(vars(.estimate), funs(mean, sd)) %>%
knitr::kable()
## Evaluate
eval <-
predictions %>%
mutate(
metrics = pred %>% map2_df(~ metrics(., truth = actual, estimate = predicted))
) %>%
select(metrics) %>%
unnest(metrics)
## Evaluate
eval <-
predictions %>%
mutate(
metrics = pred %>% map(~ metrics(., truth = actual, estimate = predicted))
) %>%
select(metrics) %>%
unnest(metrics)
eval %>% knitr::kable()
predicted
predictions
## Evaluate
eval <-
predictions %>%
mutate(
metrics = pred %>% map(~ metrics(., truth = actual, estimate = predicted))
) %>%
select(metrics) %>%
unnest(metrics)
eval %>% knitr::kable()
analysis_set <- split %>% analysis()
analysis_set
analysis_prepped <- analysis_set %>% rec
analysis_prepped <- analysis_set %>% rec()
analysis_prepped <- analysis_set %>%
recipe(Churn ~ .,) %>%
step_string2factor(all_nominal(), -all_outcomes()) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
step_center(all_numeric()) %>%
step_scale(all_numeric())
analysis_baked <- analysis_prepped %>% bake(new_data = analysis_set)
analysis_prepped <- analysis_set %>%
recipe(Churn ~ .,) %>%
step_string2factor(all_nominal(), -all_outcomes()) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
step_center(all_numeric()) %>%
step_scale(all_numeric()) %>%
prep(analysis_set)
analysis_baked <- analysis_prepped %>% bake(new_data = analysis_set)
model_rf <-
rand_forest(
mode = "classification",
mtry = try,
trees = tree
) %>%
set_engine("ranger",
importance = "impurity"
) %>%
fit(Churn ~ ., data = analysis_baked)
model_rf <-
rand_forest(
mode = "classification",
mtry = try,
trees = 200
) %>%
set_engine("ranger",
importance = "impurity"
) %>%
fit(Churn ~ ., data = analysis_baked)
model_rf <-
rand_forest(
mode = "classification",
mtry = 10,
trees = 200
) %>%
set_engine("ranger",
importance = "impurity"
) %>%
fit(Churn ~ ., data = analysis_baked)
assessment_set <- split %>% assessment()
assessment_prepped <- assessment_set %>%       recipe(Churn ~ .,) %>%
step_string2factor(all_nominal(), -all_outcomes()) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
step_center(all_numeric()) %>%
step_scale(all_numeric()) %>%
prep(analysis_set)
assessment_baked <- assessment_prepped %>% bake(new_data = assessment_set)
tibble(
"id" = id,
"truth" = assessment_baked$Churn,
"prediction" = model_rf %>%
predict(new_data = assessment_baked) %>%
unlist()
)
tibble(
"truth" = assessment_baked$Churn,
"prediction" = model_rf %>%
predict(new_data = assessment_baked) %>%
unlist()
)
recipe_rf <- function(dataset) {
recipe(Churn ~ ., data = dataset) %>%
step_string2factor(all_nominal(), -all_outcomes()) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
step_center(all_numeric()) %>%
step_scale(all_numeric()) %>%
prep(data = dataset)
}
rf_fun <- function(split, id, try, tree) {
analysis_set <- split %>% analysis()
analysis_prepped <- analysis_set %>% recipe_rf()
analysis_baked <- analysis_prepped %>% bake(new_data = analysis_set)
model_rf <-
rand_forest(
mode = "classification",
mtry = try,
trees = tree
) %>%
set_engine("ranger",
importance = "impurity"
) %>%
fit(Churn ~ ., data = analysis_baked)
assessment_set <- split %>% assessment()
assessment_prepped <- assessment_set %>% recipe_rf()
assessment_baked <- assessment_prepped %>% bake(new_data = assessment_set)
tibble(
"id" = id,
"truth" = assessment_baked$Churn,
"prediction" = model_rf %>%
predict(new_data = assessment_baked) %>%
unlist()
)
}
pred_rf <- map2_df(
.x = cross_val_tbl$splits,
.y = cross_val_tbl$id,
~ rf_fun(split = .x, id = .y, try = 3, tree = 200)
)
head(pred_rf)
pred_rf %>%
conf_mat(truth, prediction) %>%
summary() %>%
select(-.estimator) %>%
filter(.metric %in%
c("accuracy", "precision", "recall", "f_meas")) %>%
kable()
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE, dpi = 300, cache.lazy = FALSE,
tidy = "styler", fig.width = 8, fig.height = 5)
library(tidymodels)
library(skimr)
library(tibble)
library(dplyr)
library(tidyr)
library(magrittr)
telco <- readr::read_csv("WA_Fn-UseC_-Telco-Customer-Churn.csv")
telco %>%
skimr::skim()
telco <- telco %>%
select(-customerID) %>%
drop_na()
set.seed(1972)
train_test_split <- rsample::initial_split(
data = telco,
prop = 0.8
)
train_test_split
train_tbl <- train_test_split %>% training() %>%
unnest()
test_tbl <- train_test_split %>% testing()
recipe_simple <- function(dataset) {
recipe(Churn ~ ., data = dataset) %>%
step_string2factor(all_nominal(), -all_outcomes()) %>%
prep(data = dataset)
}
recipe_prepped <- recipe_simple(dataset = train_tbl)
train_baked <- bake(recipe_prepped, new_data = train_tbl)
test_baked <- bake(recipe_prepped, new_data = test_tbl)
logistic_glm <- logistic_reg(mode = "classification") %>%
set_engine("glm") %>%
fit(Churn ~ .,
data = train_baked)
prediction_glm <- logistic_glm %>%
predict(new_data = test_baked) %>%
bind_cols(test_baked %>%  select(Churn))
head(prediction_glm)
prediction_glm %>%
conf_mat(Churn, .pred_class) %>%
pluck(1) %>%
as_tibble() %>%
ggplot(aes(Prediction, Truth, alpha = n)) +
geom_tile(show.legend = FALSE) +
geom_text(aes(label = n), colour = "white", alpha = 0.5, size = 12)
prediction_glm %>%
metrics(Churn, .pred_class) %>%
select(-.estimator) %>%
filter(.metric == "accuracy")
tibble(
"precision" =
precision(prediction_glm, Churn, .pred_class) %>%
select(.estimate),
"recall" =
recall(prediction_glm, Churn, .pred_class) %>%
select(.estimate)
) %>%
unnest() %>%
kable()
prediction_glm %>%
f_meas(Churn, .pred_class) %>%
select(-.estimator) %>%
kable()
set.seed(123)
cross_val_tbl <-
vfold_cv(train_tbl, v = 10)
conf_mat(results, truth = actual, estimate = predicted)[[1]] %>%
as_tibble() %>%
ggplot(aes(Prediction, Truth, alpha = n)) +
geom_tile(fill = "blue", show.legend = FALSE) +
geom_text(aes(label = n), color = "white", alpha = 1, size = 8) +
theme_minimal() +
labs(
title = "Confusion matrix"
)
recipe_rf <- function(dataset) {
recipe(Churn ~ ., data = dataset) %>%
step_string2factor(all_nominal(), -all_outcomes()) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
step_center(all_numeric()) %>%
step_scale(all_numeric()) %>%
prep(data = dataset)
}
rf_fun <- function(split, id, try, tree) {
analysis_set <- split %>% analysis()
analysis_prepped <- analysis_set %>% recipe_rf()
analysis_baked <- analysis_prepped %>% bake(new_data = analysis_set)
model_rf <-
rand_forest(
mode = "classification",
mtry = try,
trees = tree
) %>%
set_engine("ranger",
importance = "impurity"
) %>%
fit(Churn ~ ., data = analysis_baked)
assessment_set <- split %>% assessment()
assessment_prepped <- assessment_set %>% recipe_rf()
assessment_baked <- assessment_prepped %>% bake(new_data = assessment_set)
tibble(
"id" = id,
"truth" = assessment_baked$Churn,
"prediction" = model_rf %>%
predict(new_data = assessment_baked) %>%
unlist()
)
}
pred_rf <- map2_df(
.x = cross_val_tbl$splits,
.y = cross_val_tbl$id,
~ rf_fun(split = .x, id = .y, try = 3, tree = 200)
)
head(pred_rf)
pred_rf %>%
conf_mat(truth, prediction) %>%
summary() %>%
select(-.estimator) %>%
filter(.metric %in%
c("accuracy", "precision", "recall", "f_meas")) %>%
kable()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE, dpi = 300, cache.lazy = FALSE,
tidy = "styler", fig.width = 8, fig.height = 5)
library(tidymodels)
library(skimr)
library(tibble)
library(dplyr)
library(tidyr)
library(magrittr)
data <- data %>%
rename_all(tolower)
data <- read.csv("QHP_Landscape_Individual_Market_Medical.csv", header = TRUE,
sep = ",")
data <- data %>%
rename_all(tolower)
blogdown:::new_post_addin()
View(losers)
