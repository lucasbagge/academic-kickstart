set.seed(123)
vb_split <- initial_split(vb_df, strata = win)
vb_split
set.seed(123)
# Split data
vb_split <- initial_split(vb_df, strata = win)
vb_train <- training(vb_split)
vb_test <- testing(vb_split)
xgb_spec <- boost_tree(
# skal håndtere en masse model parameter vi skal tune
# vi tuner ikke træet men sørger for der er nok af dem.
trees = 1000,
# hyper parameter
# de handler om model kompleksitet.
tree_depth = tune(), min_n = tune(), loss_reduction = tune(),
sample = tune(), mtry = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost") %>%
set_mode("classification")
xgb_spec
xgb_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
# sample størrelsen  er en andel
sample_size = sample_prop(),
# har en ukendt, ds vi ved ikke hvor mange data punkter.
mtry(),
learn_rate(),
size = 20
)
mtry()
xgb_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
# sample størrelsen  er en andel
sample_size = sample_prop(),
# har en ukendt, ds vi ved  ikke hvor mange data punkter.
#
finalize(mtry(), vb_train),
learn_rate(),
size = 20
)
xgb_grid
xgb_wf <- workflow() %>%
add_formula(win~ .) %>%
add_model(xgb_spec)
xgb_wf
vb_folds <- vfold_cv(vb_train, strata = win)
vb_folds
tune_grid(
xgb_wf,
resamples = vb_folds,
grid = xgb_grid,
control = control_grid(save_pred = TRUE)
)
xbg_res <- tune_grid(
xgb_wf,
resamples = vb_folds,
grid = xgb_grid,
control = control_grid(save_pred = TRUE)
)
doParallel::registerDoParallel()
set.seed(234)
xgb_res <- tune_grid(
xgb_wf,
resamples = vb_folds,
grid = xgb_grid,
control = control_grid(save_pred = TRUE)
)
install.packages("xgboost")
doParallel::registerDoParallel()
set.seed(234)
xgb_res <- tune_grid(
xgb_wf,
resamples = vb_folds,
grid = xgb_grid,
control = control_grid(save_pred = TRUE)
)
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE, dpi = 300, cache.lazy = FALSE,
tidy = "styler", fig.width = 8, fig.height = 5)
library(tidymodels)
library(skimr)
library(tibble)
library(dplyr)
library(tidyr)
library(magrittr)
theme_set(theme_minimal())
vb_matches <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-19/vb_matches.csv', guess_max = 76000)
vb_matches
vb_parsed <- vb_matches %>%
transmute(
circuit,
gender,
year,
# vil to hold team med større sandynlihghed veinde for nogle fejl eller sttacks
w_attacks = w_p1_tot_attacks + w_p2_tot_attacks,
w_kills = w_p1_tot_kills + w_p2_tot_kills,
w_errors = w_p1_tot_errors + w_p2_tot_errors,
w_aces = w_p1_tot_aces + w_p2_tot_aces,
w_serve_errors = w_p1_tot_serve_errors + w_p2_tot_serve_errors,
w_blocks = w_p1_tot_blocks + w_p2_tot_blocks,
w_digs = w_p1_tot_digs + w_p2_tot_digs,
l_attacks = l_p1_tot_attacks + l_p2_tot_attacks,
l_kills = l_p1_tot_kills + l_p2_tot_kills,
l_errors = l_p1_tot_errors + l_p2_tot_errors,
l_aces = l_p1_tot_aces + l_p2_tot_aces,
l_serve_errors = l_p1_tot_serve_errors + l_p2_tot_serve_errors,
l_blocks = l_p1_tot_blocks + l_p2_tot_blocks,
l_digs = l_p1_tot_digs + l_p2_tot_digs
) %>%
na.omit()
library(dplyr, warn.conflicts = FALSE)
winner <- vb_parsed %>%
select(circuit, gender, year,
w_attacks:w_digs) %>%
rename_all(function(x) gsub("w_", "", x)) %>%
mutate(win = "win")
losers <- vb_parsed %>%
select(circuit, gender, year,
l_attacks:l_digs) %>%
rename_all(function(x) gsub("l_", "", x)) %>%
mutate(win = "lose")
vb_df <- bind_rows(winner,
losers) %>%
mutate_if(is.character, factor)
vb_df %>%  count(gender)
library(tidyquant)
vb_df %>%
pivot_longer(attacks:digs, names_to = "stat", values_to = "value") %>%
ggplot(aes(gender, value, fill = win, color = win)) +
geom_boxplot(alpha = 0.4) +
facet_wrap(~ stat, scales = "free_y", nrow = 2) +
labs(y = NULL, color = NULL, fill = NULL)
set.seed(123)
# Split data
vb_split <- initial_split(vb_df, strata = win)
vb_train <- training(vb_split)
vb_test <- testing(vb_split)
xgb_spec <- boost_tree(
# skal håndtere en masse model parameter vi skal tune
# vi tuner ikke træet men sørger for der er nok af dem.
trees = 1000,
# hyper parameter
# de handler om model kompleksitet.
tree_depth = tune(), min_n = tune(), loss_reduction = tune(),
sample = tune(), mtry = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost") %>%
set_mode("classification")
xgb_spec
xgb_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
# sample størrelsen  er en andel
sample_size = sample_prop(),
# har en ukendt, ds vi ved  ikke hvor mange data punkter.
#
finalize(mtry(), vb_train),
learn_rate(),
size = 20
)
xgb_grid
xgb_wf <- workflow() %>%
add_formula(win~ .) %>%
add_model(xgb_spec)
xgb_wf
set.seed(123)
vb_folds <- vfold_cv(vb_train, strata = win)
vb_folds
doParallel::registerDoParallel()
set.seed(234)
xgb_res <- tune_grid(
xgb_wf,
resamples = vb_folds,
grid = xgb_grid,
control = control_grid(save_pred = TRUE)
)
sessionInfo()
sessionInfo(rlang)
sessionInfo()
sessionInfo()
install.packages("rlang")
install.packages("rlang")
sessionInfo()
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE, dpi = 300, cache.lazy = FALSE,
tidy = "styler", fig.width = 8, fig.height = 5)
library(tidymodels)
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE, dpi = 300, cache.lazy = FALSE,
tidy = "styler", fig.width = 8, fig.height = 5)
library(tidymodels)
install.packages("tidymodels")
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE, dpi = 300, cache.lazy = FALSE,
tidy = "styler", fig.width = 8, fig.height = 5)
library(tidymodels)
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE, dpi = 300, cache.lazy = FALSE,
tidy = "styler", fig.width = 8, fig.height = 5)
library(tidymodels)
install.packages("rlang")
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE, dpi = 300, cache.lazy = FALSE,
tidy = "styler", fig.width = 8, fig.height = 5)
library(tidymodels)
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE, dpi = 300, cache.lazy = FALSE,
tidy = "styler", fig.width = 8, fig.height = 5)
library(tidymodels)
install.packages(c("broom", "devtools", "rlang"))
install.packages(c("broom", "devtools", "rlang"))
install.packages(c("broom", "devtools", "rlang"))
install.packages(c("broom", "devtools", "rlang"))
install.packages(c("broom", "devtools", "rlang"))
install.packages(c("broom", "devtools", "rlang"))
install.packages(c("broom", "devtools", "rlang"))
install.packages("rlang")
sessionInfo()
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE, dpi = 300, cache.lazy = FALSE,
tidy = "styler", fig.width = 8, fig.height = 5)
library(tidymodels)
library(skimr)
library(tibble)
library(dplyr)
library(tidyr)
library(magrittr)
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE, dpi = 300, cache.lazy = FALSE,
tidy = "styler", fig.width = 8, fig.height = 5)
library(tidymodels)
library(skimr)
library(tibble)
library(dplyr)
library(tidyr)
library(magrittr)
#Read the data
data = read.csv("data_sales.csv",
stringsAsFactors = F)
#Fix the date
data = data %>%
mutate(InvoiceDate = as.Date(as.POSIXct(InvoiceDate,format = "%m/%d/%Y %H:%M"))) %>%
drop_na()
data %>%
skim()
#1. Get Recency, First purchase and last purchase made by customers and the total value of each transaction (QTY*PRICE)
max_InvoiceDate = max(data$InvoiceDate)
# today = max_InvoiceDate+2
data = data %>%
group_by(CustomerID) %>%
mutate(Recency = max_InvoiceDate - max(InvoiceDate),
First_purchase = max_InvoiceDate - min(InvoiceDate) ,
Value = Quantity*UnitPrice,
Frequency = n_distinct(InvoiceNo)) %>%
arrange(InvoiceDate) %>%
ungroup()
# 2. Filter out Qty < 0
neg = data %>%
filter(Quantity < 0)
data = data %>%
filter(Quantity >= 0)
# 3. Get customers with repeat transactions
data = data %>%
group_by(CustomerID) %>%
mutate(repeat_customers = ifelse(n_distinct(InvoiceNo) > 1,n_distinct(InvoiceNo),0),
churn = ifelse(repeat_customers == 0, "yes", "no")) %>%
ungroup()
# 4. Get the country with the most customer.
data = data %>%
filter(Country == "United Kingdom")
rm(max_InvoiceDate,neg)
data %>%
skim()
set.seed(1972)
train_test_split <-
initial_split(
data = data,
prop = 0.80
)
train_test_split
train_tbl <- train_test_split %>%
training()
test_tbl <- train_test_split %>%
testing()
recipe_simple <- function(dataset) {
recipe(churn ~ ., data = dataset) %>%
step_string2factor(all_nominal(),
-all_outcomes()) %>%
prep(data = dataset)
}
recipe_prepped <- recipe_simple(dataset = train_tbl)
train_baked <- bake(
recipe_prepped,
new_data = train_tbl
)
test_baked <- bake(
recipe_prepped,
new_data = test_tbl
)
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE, dpi = 300, cache.lazy = FALSE,
tidy = "styler", fig.width = 8, fig.height = 5)
library(tidymodels)
library(skimr)
library(tibble)
library(dplyr)
library(tidyr)
library(magrittr)
theme_set(theme_minimal())
vb_matches <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-19/vb_matches.csv', guess_max = 76000)
vb_matches
vb_parsed <- vb_matches %>%
transmute(
circuit,
gender,
year,
# vil to hold team med større sandynlihghed veinde for nogle fejl eller sttacks
w_attacks = w_p1_tot_attacks + w_p2_tot_attacks,
w_kills = w_p1_tot_kills + w_p2_tot_kills,
w_errors = w_p1_tot_errors + w_p2_tot_errors,
w_aces = w_p1_tot_aces + w_p2_tot_aces,
w_serve_errors = w_p1_tot_serve_errors + w_p2_tot_serve_errors,
w_blocks = w_p1_tot_blocks + w_p2_tot_blocks,
w_digs = w_p1_tot_digs + w_p2_tot_digs,
l_attacks = l_p1_tot_attacks + l_p2_tot_attacks,
l_kills = l_p1_tot_kills + l_p2_tot_kills,
l_errors = l_p1_tot_errors + l_p2_tot_errors,
l_aces = l_p1_tot_aces + l_p2_tot_aces,
l_serve_errors = l_p1_tot_serve_errors + l_p2_tot_serve_errors,
l_blocks = l_p1_tot_blocks + l_p2_tot_blocks,
l_digs = l_p1_tot_digs + l_p2_tot_digs
) %>%
na.omit()
library(dplyr, warn.conflicts = FALSE)
winner <- vb_parsed %>%
select(circuit, gender, year,
w_attacks:w_digs) %>%
rename_all(function(x) gsub("w_", "", x)) %>%
mutate(win = "win")
losers <- vb_parsed %>%
select(circuit, gender, year,
l_attacks:l_digs) %>%
rename_all(function(x) gsub("l_", "", x)) %>%
mutate(win = "lose")
vb_df <- bind_rows(winner,
losers) %>%
mutate_if(is.character, factor)
vb_df %>%  count(gender)
library(tidyquant)
vb_df %>%
pivot_longer(attacks:digs, names_to = "stat", values_to = "value") %>%
ggplot(aes(gender, value, fill = win, color = win)) +
geom_boxplot(alpha = 0.4) +
facet_wrap(~ stat, scales = "free_y", nrow = 2) +
labs(y = NULL, color = NULL, fill = NULL)
set.seed(123)
# Split data
vb_split <- initial_split(vb_df, strata = win)
vb_train <- training(vb_split)
vb_test <- testing(vb_split)
xgb_spec <- boost_tree(
# skal håndtere en masse model parameter vi skal tune
# vi tuner ikke træet men sørger for der er nok af dem.
trees = 1000,
# hyper parameter
# de handler om model kompleksitet.
tree_depth = tune(), min_n = tune(), loss_reduction = tune(),
sample = tune(), mtry = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost") %>%
set_mode("classification")
xgb_spec
xgb_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
# sample størrelsen  er en andel
sample_size = sample_prop(),
# har en ukendt, ds vi ved  ikke hvor mange data punkter.
#
finalize(mtry(), vb_train),
learn_rate(),
size = 20
)
xgb_grid
xgb_wf <- workflow() %>%
add_formula(win~ .) %>%
add_model(xgb_spec)
xgb_wf
set.seed(123)
vb_folds <- vfold_cv(vb_train, strata = win)
vb_folds
doParallel::registerDoParallel()
set.seed(234)
xgb_res <- tune_grid(
xgb_wf,
resamples = vb_folds,
grid = xgb_grid,
control = control_grid(save_pred = TRUE)
)
xgb_res
xgb_res %>%
collect_metrics()
xgb_res %>%
collect_metrics() %>%
filter(.metrics == "roc_auc") %>%
select(mean, mtry:sample_size)
xgb_res %>%
collect_metrics() %>%
filter(metrics == "roc_auc") %>%
select(mean, mtry:sample_size)
xgb_res %>%
collect_metrics() %>%
filter(metrics == "roc_auc") %>%
select(mean, mtry:sample_size)
xgb_res %>%
collect_metrics() %>%
filter(.metrics == "roc_auc") %>%
select(mean, mtry:sample_size)
xgb_res %>%
collect_metrics() %>%
filter(.metric == "roc_auc") %>%
select(mean, mtry:sample_size)
xgb_res %>%
collect_metrics() %>%
filter(.metric == "roc_auc") %>%
select(mean, mtry:sample_size) %>%
pivot_longer(mtry:sample_size,
names_to = "parameter",
values_to = "value")
xgb_res %>%
collect_metrics() %>%
filter(.metric == "roc_auc") %>%
select(mean, mtry:sample_size) %>%
pivot_longer(mtry:sample_size,
names_to = "parameter",
values_to = "value") %>%
ggplot(aes(value, mean, color = parameter)) +
geom_point(show.legend = FALSE) +
facet_wrap(~ parameter, scales ) "fee_x"
xgb_res %>%
collect_metrics() %>%
filter(.metric == "roc_auc") %>%
select(mean, mtry:sample_size) %>%
pivot_longer(mtry:sample_size,
names_to = "parameter",
values_to = "value") %>%
ggplot(aes(value, mean, color = parameter)) +
geom_point(show.legend = FALSE) +
facet_wrap(~ parameter, scales = "free_x")
show_best(xgb_res, "roc_auc")
show_best(xgb_res, "roc_auc")
show_best(xgb_res, "roc_auc") %>%
kable()
best_auv <- select_best(xgb_res, "roc_auc")
best_auv %>% kable()
final_xgb <- finalize_workflow(xgb_wf, best_auv)
final_xgb
library(vip)
final_xgb %>%
fit(data = vb_train) %>%
pull_workflow_fit() %>%
vip(geom = "point")
final_res <- last_fit(final_xgb, vb_split)
final_res <- last_fit(final_xgb, vb_split)
```
final_res <- last_fit(final_xgb, vb_split)
final_res %>%
collect_metrics
final_res %>%
collect_predictions()
final_res %>%
collect_predictions() %>%
conf_mat(win, .pred_class)
final_res %>%
collect_predictions() %>%
roc_auc(win, .pred_win)
final_res %>%
collect_predictions() %>%
roc_auc(win, .pred_win)
final_res %>%
collect_predictions() %>%
roc_auc(win, .pred_win) %>%
autoplot()
final_res %>%
collect_predictions() %>%
roc_curve(win, .pred_win) %>%
ggplot(aes(x = 1 - specificity, y = sensitivity)) +
geom_line(size = 1.5, color = "midnightblue") +
geom_abline(
lty = 2, alpha = 0.5,
color = "gray50",
size = 1.2
)
