line = list(color = '#FFFFFF', width = 1)),
showlegend = FALSE)
fig %>%
layout(title = 'United States Personal Expenditures by Categories in 1960',
xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
spam.raw %>%
mutate(TextLength = nchar(Text))
spam.raw <- spam.raw %>%
mutate(TextLength = nchar(Text))
ggplot(spam.raw, aes(x = TextLength, fill = Label)) +
geom_histogram(binwidth = 5) +
labs(y = "Text Count", x = "Length of Text",
title = "Distribution of Text Lengths with Class Labels")
spam.raw <- spam.raw %>%
mutate(TextLength = nchar(Text))
ggplot(spam.raw, aes(x = TextLength, fill = Label)) +
geom_histogram(binwidth = 5) +
labs(y = "Text Count", x = "Length of Text",
title = "Distribution of Text Lengths with Class Labels")
p <- ggplot(spam.raw, aes(x = TextLength, fill = Label)) +
geom_histogram(binwidth = 5) +
labs(y = "Text Count", x = "Length of Text",
title = "Distribution of Text Lengths with Class Labels")
ggplotly(p)
set.seed(32984)
indexes <- createDataPartition(spam.raw$Label, times = 1,
p = 0.7, list = FALSE)
train <- spam.raw[indexes,]
test <- spam.raw[-indexes,]
prop.table(table(train$Label))
prop.table(table(test$Label))
train$Text[21]
train$Text[38]
train$Text[357]
View(spam.raw)
train$Text[418]
train$Text[690]
train$Text[690]
train$Text[2267]
train$Text[2267]
train$Text[4111]
train$Text[5229]
View(train)
train$Text[191]
train$Text[518]
train %>%
filter(grepl("www", Text))
url_test <- spam.raw %>%
train %>%
filter(grepl("www", Text))
url_test <-
train %>%
filter(grepl("www", Text))
url_test %>%
head()
url_test %>%
head() %>%
kable()
train.tokens <- tokens(train$Text,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_hyphens = TRUE)
train.tokens[[518]]
train$Text[518]
train.tokens <- tokens_tolower(train.tokens)
train.tokens[[518]]
train.tokens <- tokens_select(train.tokens,
stopwords(),
selection = "remove")
train.tokens[[518]]
url_test %>%
head() %>%
kable()
train.tokens <- tokens_wordstem(train.tokens, language = "english")
train.tokens[[518]]
train.tokens.dfm <- dfm(train.tokens,
tolower = FALSE)
train.tokens.dfm
train.tokens.matrix <- as.matrix(train.tokens.dfm)
dim(train.tokens.matrix)
dim(train.tokens.matrix)
dim(train.tokens.dfm)
colnames(train.tokens.matrix)[1:50]
train.tokens.df <- cbind(Label = train$Label,
data.frame(train.tokens.dfm))
names(train.tokens.df)[c(146, 148, 235, 238)]
names(train.tokens.df) <- make.names(names(train.tokens.df))
set.seed(48743)
cv.folds <- createMultiFolds(train$Label, k = 10, times = 3)
cv.cntrl <- trainControl(method = "repeatedcv", number = 10,
repeats = 3, index = cv.folds)
library(doSNOW)
# Time the code execution
start.time <- Sys.time()
# Create a cluster to work on 10 logical cores.
cl <- makeCluster(10, type = "SOCK")
registerDoSNOW(cl)
# As our data is non-trivial in size at this point, use a single decision
# tree alogrithm as our first model. We will graduate to using more
# powerful algorithms later when we perform feature extraction to shrink
# the size of our data.
rpart.cv.1 <- train(Label ~ ., data = train.tokens.df, method = "rpart",
trControl = cv.cntrl, tuneLength = 7)
# Processing is done, stop cluster.
stopCluster(cl)
# Total time of execution on workstation was approximately 4 minutes.
total.time <- Sys.time() - start.time
total.time
# Time the code execution
start.time <- Sys.time()
# Create a cluster to work on 10 logical cores.
cl <- makeCluster(7, type = "SOCK")
registerDoSNOW(cl)
# As our data is non-trivial in size at this point, use a single decision
# tree alogrithm as our first model. We will graduate to using more
# powerful algorithms later when we perform feature extraction to shrink
# the size of our data.
rpart.cv.1 <- train(Label ~ ., data = train.tokens.df, method = "rpart",
trControl = cv.cntrl, tuneLength = 7)
# Processing is done, stop cluster.
stopCluster(cl)
# Total time of execution on workstation was approximately 4 minutes.
total.time <- Sys.time() - start.time
total.time
# Time the code execution
start.time <- Sys.time()
# Create a cluster to work on 10 logical cores.
cl <- makeCluster(5, type = "SOCK")
registerDoSNOW(cl)
# As our data is non-trivial in size at this point, use a single decision
# tree alogrithm as our first model. We will graduate to using more
# powerful algorithms later when we perform feature extraction to shrink
# the size of our data.
rpart.cv.1 <- train(Label ~ ., data = train.tokens.df, method = "rpart",
trControl = cv.cntrl, tuneLength = 7)
# Processing is done, stop cluster.
stopCluster(cl)
# Total time of execution on workstation was approximately 4 minutes.
total.time <- Sys.time() - start.time
total.time
# Time the code execution
start.time <- Sys.time()
# Create a cluster to work on 10 logical cores.
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)
# As our data is non-trivial in size at this point, use a single decision
# tree alogrithm as our first model. We will graduate to using more
# powerful algorithms later when we perform feature extraction to shrink
# the size of our data.
rpart.cv.1 <- train(Label ~ ., data = train.tokens.df, method = "rpart",
trControl = cv.cntrl, tuneLength = 7)
# Processing is done, stop cluster.
stopCluster(cl)
# Total time of execution on workstation was approximately 4 minutes.
total.time <- Sys.time() - start.time
total.time
rpart.cv.1
# Our function for calculating relative term frequency (TF)
term.frequency <- function(row) {
row / sum(row)
}
# Our function for calculating inverse document frequency (IDF)
inverse.doc.freq <- function(col) {
corpus.size <- length(col)
doc.count <- length(which(col > 0))
log10(corpus.size / doc.count)
}
# Our function for calculating TF-IDF.
tf.idf <- function(x, idf) {
x * idf
}
train.tokens.df <- apply(train.tokens.matrix, 1, term.frequency)
dim(train.tokens.df)
# Second step, calculate the IDF vector that we will use - both
# for training data and for test data!
train.tokens.idf <- apply(train.tokens.matrix, 2, inverse.doc.freq)
str(train.tokens.idf)
# Lastly, calculate TF-IDF for our training corpus.
train.tokens.tfidf <-  apply(train.tokens.df, 2, tf.idf, idf = train.tokens.idf)
dim(train.tokens.tfidf)
# Transpose the matrix
train.tokens.tfidf <- t(train.tokens.tfidf)
dim(train.tokens.tfidf)
incomplete.cases <- which(!complete.cases(train.tokens.tfidf))
train$Text[incomplete.cases]
# Fix incomplete cases
train.tokens.tfidf[incomplete.cases,] <- rep(0.0, ncol(train.tokens.tfidf))
dim(train.tokens.tfidf)
sum(which(!complete.cases(train.tokens.tfidf)))
# Make a clean data frame using the same process as before.
train.tokens.tfidf.df <- cbind(Label = train$Label, data.frame(train.tokens.tfidf))
names(train.tokens.tfidf.df) <- make.names(names(train.tokens.tfidf.df))
# Time the code execution
start.time <- Sys.time()
# Create a cluster to work on 10 logical cores.
cl <- makeCluster(5, type = "SOCK")
registerDoSNOW(cl)
# As our data is non-trivial in size at this point, use a single decision
# tree alogrithm as our first model. We will graduate to using more
# powerful algorithms later when we perform feature extraction to shrink
# the size of our data.
rpart.cv.2 <- train(Label ~ ., data = train.tokens.tfidf.df, method = "rpart",
trControl = cv.cntrl, tuneLength = 7)
# Processing is done, stop cluster.
stopCluster(cl)
# Total time of execution on workstation was
total.time <- Sys.time() - start.time
total.time
# Time the code execution
start.time <- Sys.time()
# Create a cluster to work on 10 logical cores.
cl <- makeCluster(5, type = "SOCK")
registerDoSNOW(cl)
# As our data is non-trivial in size at this point, use a single decision
# tree alogrithm as our first model. We will graduate to using more
# powerful algorithms later when we perform feature extraction to shrink
# the size of our data.
rpart.cv.2 <- train(Label ~ ., data = train.tokens.tfidf.df, method = "rpart",
trControl = cv.cntrl, tuneLength = 7)
# Processing is done, stop cluster.
stopCluster(cl)
# Total time of execution on workstation was
total.time <- Sys.time() - start.time
total.time
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE, dpi = 300, cache.lazy = FALSE,
tidy = "styler", fig.width = 8, fig.height = 5)
library(tidymodels)
library(skimr)
library(plotly)
library(tibble)
library(dplyr)
library(tidyr)
library(magrittr)
library(tidyquant)
library(ggplot2)
library(e1071)
library(caret)
library(doSNOW)
library(quanteda)
library(irlba)
library(randomForest)
theme_set(theme_minimal())
spam.raw <- read.csv("../../../tutorials-master-906d4dbc73b47d7af6f725c415896d8fd20b048c/Introduction to Text Analytics with R/spam.csv",
stringsAsFactors = FALSE,
fileEncoding = "UTF-16")
spam.raw <- spam.raw %>%
select(v1,v2) %>%
rename(Label = v1,
Text = v2) %>%
mutate(Label = as.factor(Label))
length(which(!complete.cases(spam.raw)))
p <- spam.raw %>%
count(Label)  %>%
mutate(Label = as.character(Label),
prop = prop.table(n)) %>%
mutate(ypos = cumsum(prop)- 0.5*prop)
fig <- plot_ly(
p,
labels = ~ Label,
values = ~ prop,
type = "pie",
textposition = 'inside',
textinfo = 'label+percent',
insidetextfont = list(color = '#FFFFFF'),
hoverinfo = 'text',
marker = list(colors = colors,
line = list(color = '#FFFFFF', width = 1)),
showlegend = FALSE)
fig %>%
layout(title = 'United States Personal Expenditures by Categories in 1960',
xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
highchart() %>%
hc_chart(type = "pie") %>%
hc_add_series_labels_values(labels = p$Label, values = p$prop, color = mycols)
library(highcharter)
highchart() %>%
hc_chart(type = "pie") %>%
hc_add_series_labels_values(labels = p$Label, values = p$prop, color = mycols)
?hc_add_series_labels_values
??hc_add_series_labels_values
install.packages("hightcharter")
install.packages("highcharter")
library(highcharter)
library(highcharter)
highchart() %>%
hc_chart(type = "pie") %>%
hc_add_series_labels_values(labels = p$Label, values = p$prop, color = mycols)
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE, dpi = 300, cache.lazy = FALSE,
tidy = "styler", fig.width = 8, fig.height = 5)
library(tidymodels)
library(skimr)
library(plotly)
library(tibble)
library(dplyr)
library(tidyr)
library(magrittr)
library(tidyquant)
library(ggplot2)
library(e1071)
library(caret)
library(doSNOW)
library(quanteda)
library(irlba)
library(randomForest)
theme_set(theme_minimal())
p %>%
hchart(type = "pie", hcaes(x = Label, y = prop))
p %>%
hchart(type = "pie", hcaes(x = Label, y = prop)) %>%
hc_title(text = "Marshall’s Favorite bars",
align = "center",
style = list(fontWeight = "bold", fontSize = "30px")) %>%
hc_tooltip(enabled = T) %>%
hc_subtitle(text = "In Percentage",
align = "center",
style = list(fontWeight = "bold")) %>%
hc_add_theme(hc_theme_ffx()) %>%
hc_credits(enabled = TRUE, text = "Data source:HIMYM")
p %>%
hchart(type = "pie", hcaes(x = Label, y = prop)) %>%
hc_title(text = "Marshall’s Favorite bars",
align = "center",
style = list(fontWeight = "bold", fontSize = "30px")) %>%
hc_tooltip(enabled = T) %>%
hc_subtitle(text = "In Percentage",
align = "center",
style = list(fontWeight = "bold")) %>%
hc_credits(enabled = TRUE, text = "Data source:HIMYM")
p %>%
hchart(type = "pie", hcaes(x = Label, y = prop)) %>%
hc_title(text = "Marshall’s Favorite bars",
align = "center",
style = list(fontWeight = "bold", fontSize = "30px")) %>%
hc_tooltip(enabled = T) %>%
hc_subtitle(text = "In Percentage",
align = "center",
style = list(fontWeight = "bold")) %>%
hc_add_theme(hc_theme_economist()) %>%
hc_credits(enabled = TRUE, text = "Data source:HIMYM")
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE, dpi = 300, cache.lazy = FALSE,
tidy = "styler", fig.width = 8, fig.height = 5)
library(tidymodels)
library(skimr)
library(plotly)
library(tibble)
library(dplyr)
library(tidyr)
library(magrittr)
library(tidyquant)
library(ggplot2)
library(e1071)
library(caret)
library(doSNOW)
library(quanteda)
library(irlba)
library(randomForest)
theme_set(theme_minimal())
spam.raw <- read.csv("../../../tutorials-master-906d4dbc73b47d7af6f725c415896d8fd20b048c/Introduction to Text Analytics with R/spam.csv",
stringsAsFactors = FALSE,
fileEncoding = "UTF-16")
spam.raw <- spam.raw %>%
select(v1,v2) %>%
rename(Label = v1,
Text = v2) %>%
mutate(Label = as.factor(Label))
length(which(!complete.cases(spam.raw)))
mycols <- c("#0073C2FF", "#EFC000FF")
library(highcharter)
p %>%
hchart(type = "pie", hcaes(x = Label, y = prop)) %>%
hc_title(text = "Marshall’s Favorite bars",
align = "center",
style = list(fontWeight = "bold", fontSize = "30px")) %>%
hc_tooltip(enabled = T) %>%
hc_subtitle(text = "In Percentage",
align = "center",
style = list(fontWeight = "bold")) %>%
hc_add_theme(hc_theme_economist()) %>%
hc_credits(enabled = TRUE, text = "Data source:HIMYM")
spam.raw <- spam.raw %>%
mutate(TextLength = nchar(Text))
p <- ggplot(spam.raw, aes(x = TextLength, fill = Label)) +
geom_histogram(binwidth = 5) +
labs(y = "Text Count", x = "Length of Text",
title = "Distribution of Text Lengths with Class Labels")
ggplotly(p)
set.seed(32984)
indexes <- createDataPartition(spam.raw$Label,
times = 1,
p = 0.7,
list = FALSE)
train <- spam.raw[indexes,]
test <- spam.raw[-indexes,]
prop.table(table(train$Label))
prop.table(table(test$Label))
train$Text[518]
url_test <-
train %>%
filter(grepl("www", Text))
url_test %>%
head() %>%
kable()
train.tokens <- tokens(train$Text,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_hyphens = TRUE)
train.tokens[[518]]
train.tokens <- tokens_tolower(train.tokens)
train.tokens[[518]]
train.tokens <- tokens_select(train.tokens,
stopwords(),
selection = "remove")
train.tokens[[518]]
train.tokens <- tokens_wordstem(train.tokens,
language = "english")
train.tokens[[518]]
train.tokens.dfm <- dfm(train.tokens,
tolower = FALSE)
train.tokens.matrix <- as.matrix(train.tokens.dfm)
dim(train.tokens.matrix)
train.tokens.df <- cbind(Label = train$Label,
data.frame(train.tokens.dfm))
names(train.tokens.df)[c(146, 148, 235, 238)]
names(train.tokens.df) <- make.names(names(train.tokens.df))
set.seed(48743)
cv.folds <- createMultiFolds(train$Label, k = 10, times = 3)
cv.cntrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
index = cv.folds)
# Time the code execution
start.time <- Sys.time()
# Create a cluster to work on 10 logical cores.
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)
# As our data is non-trivial in size at this point, use a single decision
# tree alogrithm as our first model. We will graduate to using more
# powerful algorithms later when we perform feature extraction to shrink
# the size of our data.
rpart.cv.1 <- train(Label ~ ., data = train.tokens.df, method = "rpart",
trControl = cv.cntrl, tuneLength = 7)
# Processing is done, stop cluster.
stopCluster(cl)
# Total time of execution on workstation was approximately 4 minutes.
total.time <- Sys.time() - start.time
total.time
rpart.cv.1
# Our function for calculating relative term frequency (TF)
term.frequency <- function(row) {
row / sum(row)
}
# Our function for calculating inverse document frequency (IDF)
inverse.doc.freq <- function(col) {
corpus.size <- length(col)
doc.count <- length(which(col > 0))
log10(corpus.size / doc.count)
}
# Our function for calculating TF-IDF.
tf.idf <- function(x, idf) {
x * idf
}
train.tokens.df <- apply(train.tokens.matrix, 1, term.frequency)
dim(train.tokens.df)
# Second step, calculate the IDF vector that we will use - both
# for training data and for test data!
train.tokens.idf <- apply(train.tokens.matrix, 2, inverse.doc.freq)
str(train.tokens.idf)
# Lastly, calculate TF-IDF for our training corpus.
train.tokens.tfidf <-  apply(train.tokens.df, 2, tf.idf, idf = train.tokens.idf)
dim(train.tokens.tfidf)
# Transpose the matrix
train.tokens.tfidf <- t(train.tokens.tfidf)
dim(train.tokens.tfidf)
incomplete.cases <- which(!complete.cases(train.tokens.tfidf))
train$Text[incomplete.cases]
# Fix incomplete cases
train.tokens.tfidf[incomplete.cases,] <- rep(0.0, ncol(train.tokens.tfidf))
dim(train.tokens.tfidf)
sum(which(!complete.cases(train.tokens.tfidf)))
# Make a clean data frame using the same process as before.
train.tokens.tfidf.df <- cbind(Label = train$Label, data.frame(train.tokens.tfidf))
names(train.tokens.tfidf.df) <- make.names(names(train.tokens.tfidf.df))
# Time the code execution
start.time <- Sys.time()
# Create a cluster to work on 10 logical cores.
cl <- makeCluster(5, type = "SOCK")
registerDoSNOW(cl)
# As our data is non-trivial in size at this point, use a single decision
# tree alogrithm as our first model. We will graduate to using more
# powerful algorithms later when we perform feature extraction to shrink
# the size of our data.
rpart.cv.2 <- train(Label ~ ., data = train.tokens.tfidf.df, method = "rpart",
trControl = cv.cntrl, tuneLength = 7)
# Processing is done, stop cluster.
stopCluster(cl)
# Total time of execution on workstation was
total.time <- Sys.time() - start.time
total.time
install.packages("gefionr")
install.packages("gefionr")
